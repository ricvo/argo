{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0120 12:44:05.194743 140215686801216 deprecation_wrapper.py:119] From /data1/env/tf1.14.0/lib/python3.6/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0120 12:44:06.220829 140215686801216 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0120 12:44:06.649725 140215686801216 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/hooks/ArgoHook.py:25: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W0120 12:44:06.782764 140215686801216 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/hooks/CheckpointSaverHook.py:7: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
      "\n",
      "W0120 12:44:06.783932 140215686801216 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/hooks/CheckpointSaverHook.py:13: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from datasets.Dataset import Dataset\n",
    "from argo.core.ArgoLauncher import ArgoLauncher\n",
    "from argo.core.TFDeepLearningModel import load_model, load_network, load_model_without_session\n",
    "from argo.core.utils.argo_utils import load_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43ebfc4438>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.  0.  1.]\n",
      "  [ 2.  3.  4.]\n",
      "  [ 5.  6.  7.]\n",
      "  [ 8.  9. 10.]\n",
      "  [11. 12. 13.]\n",
      "  [14. 15. 16.]\n",
      "  [17. 18. 19.]\n",
      "  [20. 21. 22.]\n",
      "  [23. 24. 25.]\n",
      "  [26. 27. 28.]]\n",
      "\n",
      " [[29. 30. 31.]\n",
      "  [32. 33. 34.]\n",
      "  [35. 36. 37.]\n",
      "  [38. 39. 40.]\n",
      "  [41. 42. 43.]\n",
      "  [44. 45. 46.]\n",
      "  [47. 48. 49.]\n",
      "  [50. 51. 52.]\n",
      "  [53. 54. 55.]\n",
      "  [56. 57. 58.]]] (2, 10, 3)\n",
      "add\n",
      "[[[-1.  0.  1.]\n",
      "  [ 2.  3.  4.]\n",
      "  [ 5.  6.  7.]\n",
      "  [ 8.  9. 10.]\n",
      "  [11. 12. 13.]\n",
      "  [14. 15. 16.]\n",
      "  [17. 18. 19.]\n",
      "  [20. 21. 22.]\n",
      "  [23. 24. 25.]\n",
      "  [26. 27. 28.]]\n",
      "\n",
      " [[29. 30. 31.]\n",
      "  [32. 33. 34.]\n",
      "  [35. 36. 37.]\n",
      "  [38. 39. 40.]\n",
      "  [41. 42. 43.]\n",
      "  [44. 45. 46.]\n",
      "  [47. 48. 49.]\n",
      "  [50. 51. 52.]\n",
      "  [53. 54. 55.]\n",
      "  [56. 57. 58.]]]\n"
     ]
    }
   ],
   "source": [
    "# Import `tensorflow`\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "# Initialize two constants\n",
    "# Import `tensorflow`\n",
    "\n",
    "# Initialize two constants\n",
    "a=np.arange(1,61,1)\n",
    "#print(a)\n",
    "x1=tf.constant(a, shape=(2,10, 3),dtype=tf.float32) \n",
    "tie=tf.tile(x1,[1,1,1])\n",
    "x2= tf. add(tie,-2)\n",
    "reshape= tf.reshape(x2, (1,2,10,3)) \n",
    "average=tf.reduce_mean(reshape,axis=0)\n",
    "# Initialize Session and run `result`\n",
    "with tf.Session() as sess:\n",
    "  output = sess.run(x2)\n",
    "  average = sess.run(average)\n",
    "  print(output,output.shape)\n",
    "  print('add')\n",
    "  print(average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Normal_27/log_prob/sub:0\", shape=(2, 10, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Define a single scalar Normal distribution.\n",
    "dist = tfd.Normal(loc=0., scale=3.)\n",
    "\n",
    "# Evaluate the cdf at 1, returning a scalar.\n",
    "dist.cdf(1.)\n",
    "\n",
    "# Define a batch of two scalar valued Normals.\n",
    "# The first has mean 1 and standard deviation 11, the second 2 and 22.\n",
    "dist = tfd.Normal(loc=[1, 2.,1.5], scale=[11, 22,.12])\n",
    "dist1=dist.log_prob(x2)\n",
    "#dist1=tf.pow(dist1,1)\n",
    "# Evaluate the pdf of the first distribution on 0, and the second on 1.5,\n",
    "# returning a length two tensor.\n",
    "print(dist1)\n",
    "\n",
    "\n",
    "# Get 3 samples, returning a 3 x 2 tensor.\n",
    "#dist.sample([3])\n",
    "#with tf.Session() as sess:\n",
    "  #output = sess.run(dist1)\n",
    "#print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SESSION\n",
    "tf.set_random_seed(1000)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config = sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB i saved the model every 20 epochs\n",
    "#ff_global_step = 486000\n",
    "# ffconffile = \"/data1/mcmc/sept_drop/NNewDroo/CMB-d%50-n11/FF-cLL-st0-stp0-bs32-trA_lrm1e-09E.i1e-04.s3e+03.r0.9_bo0.9_bt0.999-cGN100-VGGB_dc0_dr0.05_bnm0.99_bnr0_TriL_mc0.0001-aLR-wign-bic1.0-wrLtwo0.003-brLtwo0.0001-r0/experiment.conf\"\n",
    "# ffconffile = \"/data/captamerica_hd1/mcmc/freeze_staircase/CMB-d%100-n11/FF-cLL-st0-stp0-bs32-trA_lrm1e-09E.i1e-04.s6e+03.r0.9_bo0.9_bt0.999-cGN100-BVGG_fl1_rn1_f16.16.32.32.32_mp-TriL_b0_fl1-LIgn-c10.0-klrLtwo1e-05-ksrLtwo1e-05-blrLtwo1e-05-SCM-1.6-PSc1.0-tr1-r0/experiment.conf\"\n",
    "ffconffile = '/data/ironman_hd1/mcmc/bayesian_Pol/CMB-d%100-n11/FF-cLL-st0-stp0-bs32-trA_lrm1e-09E.i1e-04.s6e+03.r0.9_bo0.9_bt0.999-cGN100-BVGG_fl1_rn1_f16.16.32.32.32_mp-TriL_b0_fl1-LIgn-c10-klrLtwo1e-05-ksrLtwo1e-05-blrLtwo1e-05-SCM-1.6-PSc1.0-tr1-r0/experiment.conf'\n",
    "ff_global_step = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_dataset_conf, ff_model_parameters, ff_config = ArgoLauncher.process_conf_file(ffconffile)\n",
    "# dataset = Dataset.load_dataset(ff_dataset_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200 are going to be loaded in memory\n",
      "4800 are going to be loaded in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 18:28:59.504005 139932174079744 deprecation_wrapper.py:119] From /data1/env/tf1.14.0/lib/python3.5/site-packages/sonnet/python/modules/base.py:177: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n",
      "W1203 18:28:59.787890 139932174079744 deprecation.py:506] From /data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1203 18:28:59.809685 139932174079744 deprecation_wrapper.py:119] From /data1/env/tf1.14.0/lib/python3.5/site-packages/sonnet/python/modules/base.py:278: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1203 18:28:59.813550 139932174079744 deprecation_wrapper.py:119] From /data1/env/tf1.14.0/lib/python3.5/site-packages/sonnet/python/modules/base.py:579: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1203 18:28:59.818127 139932174079744 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/keras_models/keras_utils.py:243: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "W1203 18:28:59.849050 139932174079744 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/keras_models/keras_utils.py:249: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n",
      "W1203 18:28:59.857327 139932174079744 deprecation.py:323] From /data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W1203 18:29:00.111424 139932174079744 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/TFDeepLearningModel.py:217: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W1203 18:29:00.217120 139932174079744 deprecation.py:323] From /home/hector/prediction/datasets/CMB_Pol.py:230: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W1203 18:29:00.321266 139932174079744 deprecation.py:323] From /data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1203 18:29:00.341142 139932174079744 deprecation.py:323] From /home/hector/prediction/datasets/Dataset.py:384: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W1203 18:29:00.448140 139932174079744 deprecation.py:323] From /home/hector/prediction/datasets/Dataset.py:388: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "W1203 18:29:00.731744 139932174079744 deprecation.py:323] From /home/hector/prediction/datasets/Dataset.py:541: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
      "W1203 18:29:00.732758 139932174079744 deprecation.py:323] From /home/hector/prediction/datasets/Dataset.py:548: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
      "W1203 18:29:00.735683 139932174079744 deprecation_wrapper.py:119] From /home/hector/prediction/datasets/Dataset.py:576: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "W1203 18:29:00.744834 139932174079744 deprecation_wrapper.py:119] From /home/hector/prediction/core/argo/core/optimizers/LearningRates.py:46: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 are going to be loaded in memory\n",
      "found 35 layers\n",
      "found 4 layers\n",
      "['flatten',\n",
      " 'dense',\n",
      " 'dense_1',\n",
      " 'dense_2',\n",
      " 'conv2d_flipout',\n",
      " 'batch_normalization',\n",
      " 'leaky_re_lu',\n",
      " 'conv2d_flipout_1',\n",
      " 'batch_normalization_1',\n",
      " 'leaky_re_lu_1',\n",
      " 'max_pooling2d',\n",
      " 'conv2d_flipout_2',\n",
      " 'batch_normalization_2',\n",
      " 'leaky_re_lu_2',\n",
      " 'conv2d_flipout_3',\n",
      " 'batch_normalization_3',\n",
      " 'leaky_re_lu_3',\n",
      " 'max_pooling2d_1',\n",
      " 'conv2d_flipout_4',\n",
      " 'batch_normalization_4',\n",
      " 'leaky_re_lu_4',\n",
      " 'conv2d_flipout_5',\n",
      " 'batch_normalization_5',\n",
      " 'leaky_re_lu_5',\n",
      " 'max_pooling2d_2',\n",
      " 'conv2d_flipout_6',\n",
      " 'batch_normalization_6',\n",
      " 'leaky_re_lu_6',\n",
      " 'conv2d_flipout_7',\n",
      " 'batch_normalization_7',\n",
      " 'leaky_re_lu_7',\n",
      " 'max_pooling2d_3',\n",
      " 'conv2d_flipout_8',\n",
      " 'batch_normalization_8',\n",
      " 'leaky_re_lu_8',\n",
      " 'conv2d_flipout_9',\n",
      " 'batch_normalization_9',\n",
      " 'leaky_re_lu_9',\n",
      " 'max_pooling2d_4']\n",
      "\n",
      "found 10 BatchNorm layers\n",
      "found 10 Flipout layers\n",
      "found 0 Reparameterization layers\n",
      "\n",
      "found 36 keras regularizers\n",
      "[<tf.Tensor 'ff_network_1/TriL/dense/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/TriL/dense/bias/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/TriL/dense_1/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/TriL/dense_1/bias/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/TriL/dense_2/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/TriL/dense_2/bias/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_1/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_1/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_1/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_2/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_2/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_2/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_3/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_3/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_3/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_4/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_4/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_4/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_5/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_5/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_5/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_6/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_6/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_6/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_7/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_7/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_7/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_8/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_8/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_8/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_9/kernel_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_9/kernel_posterior_untransformed_scale/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_9/bias_posterior_loc/Regularizer/l2_regularizer:0' shape=() dtype=float32>]\n",
      "\n",
      "found 10 keras kl losses\n",
      "[<tf.Tensor 'ff_network_1/vgg/conv2d_flipout/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_1/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_2/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_3/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_4/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_5/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_6/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_7/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_8/divergence_kernel:0' shape=() dtype=float32>,\n",
      " <tf.Tensor 'ff_network_1/vgg/conv2d_flipout_9/divergence_kernel:0' shape=() dtype=float32>]\n",
      "\n",
      "found 20 keras update ops\n",
      "[<tf.Operation 'ff_network_1/vgg/batch_normalization/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_1/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_1/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_2/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_2/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_3/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_3/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_4/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_4/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_5/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_5/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_6/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_6/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_7/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_7/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_8/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_8/cond_7/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_9/cond_6/Merge' type=Merge>,\n",
      " <tf.Operation 'ff_network_1/vgg/batch_normalization_9/cond_7/Merge' type=Merge>]\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE MODEL BUT NOT THE MONITORED SESSION SINCE WE WANT TO CONTROL THE SESSION\n",
    "model, dataset, checkpoint_name = load_model_without_session(ffconffile,global_step=ff_global_step, model_class_base_path=\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/ironman_hd1/mcmc/bayesian_Pol/CMB-d%100-n11/FF-cLL-st0-stp0-bs32-trA_lrm1e-09E.i1e-04.s6e+03.r0.9_bo0.9_bt0.999-cGN100-BVGG_fl1_rn1_f16.16.32.32.32_mp-TriL_b0_fl1-LIgn-c10-klrLtwo1e-05-ksrLtwo1e-05-blrLtwo1e-05-SCM-1.6-PSc1.0-tr1-r0/saved_models/model.ckpt-162000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ff_network_1/TriL/MultivariateNormalTriL/mean/add:0' shape=(?, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prediction_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inputs:0' shape=(?, 256, 256, 3) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = model.is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "#before any form of preprocessing and augmentation (if present)\n",
    "raw_x = model.raw_x\n",
    "x = model.x\n",
    "y = model.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mu_bar_sigma_bar(x, network, dim, nsamples=5, scope=\"bar\"):\n",
    "    \n",
    "    with tf.name_scope(scope) as scope: \n",
    "    \n",
    "        dtype = x.dtype\n",
    "        \n",
    "        empty = np.transpose(np.array([[],]*dim), [1,0])\n",
    "        samples_stack = tf.constant(empty, dtype=dtype)\n",
    "        \n",
    "        i = tf.constant(0, dtype=tf.int32, name=\"i0\")\n",
    "        \n",
    "        def cond(i, samples_stack):\n",
    "            return i < nsamples\n",
    "\n",
    "        \n",
    "        def body(i, samples_stack):\n",
    "            distr = network(x)\n",
    "            samples_stack = tf.concat([samples_stack, distr.sample()], axis=0)\n",
    "            return i+1, samples_stack\n",
    "        \n",
    "        i = tf.constant(0)\n",
    "        i, samples_stack = tf.while_loop(cond,\n",
    "                                        body,\n",
    "                                        loop_vars=[i, samples_stack],\n",
    "                                        parallel_iterations=10,\n",
    "                                        shape_invariants=[i.get_shape(), tf.TensorShape([None, dim])])\n",
    "\n",
    "    return samples_stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 10\n",
    "x_repeat = tf.tile(x, [nsamples, 1, 1, 1])\n",
    "repeated_distr = model._network(x_repeat, is_training=model.is_training)\n",
    "# repeated_samples = create_mu_bar_sigma_bar(x, model._network, model.prediction_mean.shape[1], nsamples=nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 18:29:22.686587 139932174079744 deprecation.py:323] From /data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I1203 18:29:22.690189 139932174079744 saver.py:1280] Restoring parameters from /data/ironman_hd1/mcmc/bayesian_Pol/CMB-d%100-n11/FF-cLL-st0-stp0-bs32-trA_lrm1e-09E.i1e-04.s6e+03.r0.9_bo0.9_bt0.999-cGN100-BVGG_fl1_rn1_f16.16.32.32.32_mp-TriL_b0_fl1-LIgn-c10-klrLtwo1e-05-ksrLtwo1e-05-blrLtwo1e-05-SCM-1.6-PSc1.0-tr1-r0/saved_models/model.ckpt-162000\n"
     ]
    }
   ],
   "source": [
    "# RESTORE THE WEIGHTS\n",
    "saver = tf.train.Saver(None, max_to_keep=None, save_relative_paths=True)\n",
    "saver.restore(sess, checkpoint_name)\n",
    "# TEST THAT ALL WAS FINE (if weights are not restore will complain that they are uninitialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argo.core.network.KerasNetwork import KerasNetwork\n",
    "\n",
    "# NETWORK\n",
    "network = model._network\n",
    "\n",
    "if isinstance(network, KerasNetwork):\n",
    "    # FOR KERAS\n",
    "    net_model = model._network._net_model\n",
    "    distr_model = model._network._distr_model\n",
    "else:\n",
    "    # FOR SONNET\n",
    "    distr_model = model._network.module._distr_model\n",
    "\n",
    "calibration_tril = distr_model.calibration_tril\n",
    "calibration_tril_params = distr_model._calibration_tril_params\n",
    "distr = model.prediction_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<core.argo.core.keras_models.MultivariateNormalTriL.MultivariateNormalTriL at 0x7f4408779908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_vars_for_3 = distr_model.dense_loc.weights+[calibration_tril_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'ff_network_1/TriL/dense/kernel:0' shape=(2048, 3) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense/bias:0' shape=(3,) dtype=float32>,\n",
       " <tf.Variable 'ff_network/calibration_tril_params:0' shape=(6,) dtype=float32>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_vars_for_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'ff_network_1/TriL/dense/kernel:0' shape=(2048, 3) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense/bias:0' shape=(3,) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense_1/kernel:0' shape=(2048, 3) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense_1/bias:0' shape=(3,) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense_2/kernel:0' shape=(2048, 3) dtype=float32>,\n",
       " <tf.Variable 'ff_network_1/TriL/dense_2/bias:0' shape=(3,) dtype=float32>,\n",
       " <tf.Variable 'ff_network/calibration_tril_params:0' shape=(6,) dtype=float32>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]], dtype=float32),\n",
       " array([1., 1., 1., 1., 1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([calibration_tril, calibration_tril_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 492 ms, total: 2.78 s\n",
      "Wall time: 7.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np_samples = [] \n",
    "for i in range(nsamples):\n",
    "    np_samples.append(sess.run(distr.parameters['loc'], feed_dict={model.x: np.ones((1, 256, 256, 3))}))\n",
    "\n",
    "np_samples = np.squeeze(np.stack(np_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3) [-0.870959   -1.8646564   0.87731713]\n"
     ]
    }
   ],
   "source": [
    "print(np_samples.shape, np.mean(np_samples, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 440 ms, sys: 48 ms, total: 488 ms\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_samples = sess.run(repeated_distr.parameters['loc'], feed_dict={model.x: np.ones((1, 256, 256, 3))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3) [-0.87007445 -1.8737339   0.8546548 ]\n"
     ]
    }
   ],
   "source": [
    "print(tf_samples.shape, np.mean(tf_samples, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_shuffled': <tf.Operation 'MakeIterator_5' type=MakeIterator>, 'validation': <tf.Operation 'MakeIterator_1' type=MakeIterator>, 'train': <tf.Operation 'MakeIterator' type=MakeIterator>, 'validation_shuffled': <tf.Operation 'MakeIterator_4' type=MakeIterator>, 'test': <tf.Operation 'MakeIterator_2' type=MakeIterator>, 'train_shuffled': <tf.Operation 'MakeIterator_3' type=MakeIterator>}\n",
      "{'validation_shuffled': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x0f_4_IteratorV2_4 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'test_shuffled': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x0f_5_IteratorV2_5 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'validation': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x0f_3_IteratorV2_1 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'train': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\r_2_IteratorV2 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'train_loop': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x12_6_OneShotIterator \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'test': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x0f_1_IteratorV2_2 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE', 'train_shuffled': b'\\n,/job:localhost/replica:0/task:0/device:CPU:0\\x12\\tlocalhost\\x1a\\x0f_0_IteratorV2_3 \\x88\\xca\\x87\\xfe\\xca\\xce\\x8d\\xf5:*%N10tensorflow4data16IteratorResourceE'}\n"
     ]
    }
   ],
   "source": [
    "# DATASET SECTION\n",
    "\n",
    "# these are all the datasets tag available\n",
    "from datasets.Dataset import TRAIN_LOOP, TRAIN, VALIDATION, TEST, \\\n",
    "                TRAIN_SHUFFLED, VALIDATION_SHUFFLED, TEST_SHUFFLED\n",
    "\n",
    "datasets_initializers = model.datasets_initializers\n",
    "ds_handle = model.ds_handle\n",
    "datasets_handles = sess.run(model.datasets_handles_nodes)\n",
    "print(datasets_initializers)\n",
    "print(datasets_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1203 13:08:01.183656 139830050121472 deprecation.py:323] From /data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow_probability/python/stats/sample_stats.py:693: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n"
     ]
    }
   ],
   "source": [
    "bs = tf.shape(distr.parameters['loc'])[0]\n",
    "ps = distr.parameters['loc'].shape[1]\n",
    "repeated_sample = tf.reshape(repeated_distr.sample(), [-1,bs,ps])\n",
    "mu_bar = tf.reduce_mean(repeated_sample, axis=0)\n",
    "sigma_bar = tfp.stats.covariance(repeated_sample, sample_axis=0, event_axis=-1)\n",
    "calibration_scalar_bar = tf.get_variable(\"calibration_scalar_bar\",\n",
    "                                    shape = (),\n",
    "                                    dtype = mu_bar.dtype,\n",
    "                                    initializer=tf.initializers.constant(1.))\n",
    "\n",
    "distr_bar = tfp.distributions.MultivariateNormalFullCovariance(loc = mu_bar, covariance_matrix = calibration_scalar_bar*sigma_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8087219 , -1.888662  ,  0.89293706],\n",
       "       [-0.83620226, -1.8956339 ,  0.8148216 ],\n",
       "       [-0.86583644, -1.8834718 ,  0.7882209 ],\n",
       "       [-0.7660073 , -1.9089873 ,  0.78894943],\n",
       "       [-0.80293655, -1.8970101 ,  0.8587028 ],\n",
       "       [-0.88309133, -1.8879697 ,  0.802112  ],\n",
       "       [-0.9110376 , -1.884928  ,  0.9219307 ],\n",
       "       [-0.8600849 , -1.8402784 ,  0.9237768 ],\n",
       "       [-0.87770146, -1.8761364 ,  0.83460844],\n",
       "       [-0.8920382 , -1.8913221 ,  0.8878106 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(mu_bar, feed_dict={model.x: np.ones((10, 256, 256, 3))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "loc = distr.parameters[\"loc\"]\n",
    "scale_tril = distr.parameters[\"scale_tril\"]\n",
    "\n",
    "calibration_scalar = tf.get_variable(\"calibration_scalar\",\n",
    "                                    shape = (),\n",
    "                                    dtype = loc.dtype,\n",
    "                                    initializer=tf.initializers.constant(1.))\n",
    "\n",
    "sc_distr = tfp.distributions.MultivariateNormalTriL(loc = loc, scale_tril = calibration_scalar*scale_tril)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable cal_global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-32-2101a4a61b5a>\", line 6, in <module>\n    initializer=tf.initializers.constant(0))\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2101a4a61b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     initializer=tf.initializers.constant(0))\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m/data1/env/tf1.14.0/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable cal_global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-32-2101a4a61b5a>\", line 6, in <module>\n    initializer=tf.initializers.constant(0))\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/data1/env/tf1.14.0/lib/python3.5/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL STEP\n",
    "global_step_restore_int = sess.run(model.global_step)\n",
    "cal_global_step = tf.get_variable(\"cal_global_step\",\n",
    "                                    shape = (),\n",
    "                                    dtype = tf.int64,\n",
    "                                    initializer=tf.initializers.constant(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE OPTIMIZER\n",
    "optimizer_tuple = (\"AdamOptimizer\", {\"learning_rate\" :\n",
    "                                        (1e-6,\n",
    "                                        \"exponential_decay\", {\"learning_rate\" : 1e-4,\n",
    "                                                              \"decay_steps\" : 6000, # frac: 100->100000, 60->50000, 50->40000\n",
    "                                                              # total train samples: 43200. 43200/32 = 1350 steps per epoch\n",
    "                                                              \"decay_rate\" : 0.9,\n",
    "                                                              \"staircase\" : True\n",
    "                                                             }\n",
    "                                         ),\n",
    "                                  \"beta1\" : 0.9,\n",
    "                                  \"beta2\" : 0.999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argo.core.optimizers.LearningRates import process_learning_rate\n",
    "from argo.core.utils.argo_utils import eval_method_from_tuple\n",
    "\n",
    "optimizer_name = optimizer_tuple[0]\n",
    "optimizer_kwargs = optimizer_tuple[1]\n",
    "\n",
    "lr = process_learning_rate(optimizer_kwargs[\"learning_rate\"], cal_global_step)\n",
    "optimizer_kwargs[\"learning_rate\"] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_op(optimizer, total_loss, global_step, var_list=None, update_ops=[], clip_value=100):\n",
    "    # 1st part of minimize: compute_gradient\n",
    "    grads_and_vars = optimizer.compute_gradients(total_loss, var_list=var_list)\n",
    "\n",
    "    # clip gradients\n",
    "    grads_and_vars_not_none = [(g, v) for (g, v) in grads_and_vars if g is not None]\n",
    "    grads = [g for (g, v) in grads_and_vars_not_none]\n",
    "    variables = [v for (g, v) in grads_and_vars_not_none]\n",
    "    clipped_grads, global_norm = tf.clip_by_global_norm(grads, clip_value)\n",
    "    clipped_grads_and_vars = [(clipped_grads[i], variables[i]) for i in range(len(grads))]    \n",
    "    \n",
    "    # 2nd part of minimize: apply_gradient\n",
    "    optimizer_step = optimizer.apply_gradients(clipped_grads_and_vars, global_step=global_step)\n",
    "    \n",
    "    update_ops = tf.group(*update_ops)\n",
    "    training_op = tf.group(update_ops, optimizer_step)\n",
    "    return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_epoch(training_op, nodes, DATASET_STR):\n",
    "    sess.run(datasets_initializers[DATASET_STR])\n",
    "    while True:\n",
    "        try:\n",
    "            _, nodes_np = sess.run([training_op,\n",
    "                                        nodes],\n",
    "                                        feed_dict={\n",
    "                                            ds_handle : datasets_handles[DATASET_STR]\n",
    "                                        }\n",
    "                                       )\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "\n",
    "# def create_mu_bar_sigma_bar(x, distr, nsamples=5, scope=\"bar\"):\n",
    "#     loc = distr.parameters[\"loc\"]\n",
    "#     scale_tril = distr.parameters[\"scale_tril\"]\n",
    "#     cov = tf.einsum(\"bij,bjk->bik\", scale_tril, tf.transpose(scale_tril, perm=[0,2,1]))\n",
    "# #     n_samples = tf.placeholder_with_default(n_samples,)\n",
    "    \n",
    "#     x_repeat = tf.tile(x, [nsamples, 1, 1, 1])\n",
    "    \n",
    "#     dim = loc.shape[1]\n",
    "    \n",
    "#     with tf.variable_scope(scope) as scope: \n",
    "    \n",
    "#         loc_shape = loc.shape.as_list()\n",
    "#         if len(loc_shape) != 2:\n",
    "#             raise RuntimeError(\"loc is expected to be a 2D tensor. Found shape {:}\".format(loc_shape))\n",
    "\n",
    "#         cov_shape = cov.shape.as_list()\n",
    "#         if len(cov_shape) != 3:\n",
    "#             raise RuntimeError(\"cov is expected to be a 3D tensor. Found shape {:}\".format(cov_shape))\n",
    "        \n",
    "#         dtype = loc.dtype\n",
    "        \n",
    "#         empty = np.transpose(np.array([[],]*dim), [1,0])\n",
    "#         mu_stack = tf.constant(empty, dtype=dtype)\n",
    "\n",
    "#         empty = np.transpose(np.array([[[],]*dim]*dim), [2,1,0])\n",
    "#         sigma_stack = tf.constant(empty, dtype=dtype)\n",
    "        \n",
    "#         i = tf.constant(0, dtype=tf.int32, name=\"i0\")\n",
    "        \n",
    "#         def cond(i, mu_stack, sigma_stack):\n",
    "#             return i < nsamples\n",
    "\n",
    "        \n",
    "#         def body(i, mu_stack, sigma_stack):\n",
    "#             mu_stack = tf.concat([mu_stack, loc], axis=0)\n",
    "#             sigma_stack = tf.concat([sigma_stack, cov], axis=0)\n",
    "#             return i+1, mu_stack, sigma_stack\n",
    "        \n",
    "#         i = tf.constant(0)\n",
    "#         i, mu_stack, sigma_stack = tf.while_loop(cond,\n",
    "#                                                 body,\n",
    "#                                                 loop_vars=[i, mu_stack, sigma_stack],\n",
    "#                                                 parallel_iterations=10,\n",
    "#                                                 shape_invariants=[i.get_shape(), tf.TensorShape([None, dim]), tf.TensorShape([None, dim, dim])])\n",
    "\n",
    "#     return x_repeat, mu_stack, sigma_stack\n",
    "\n",
    "\n",
    "# def create_mu_bar_sigma_bar_samples(x, distr, nsamples=5, scope=\"bar\"):\n",
    "#     samples = tf.stack([distr.sample() for s in range(nsamples)])\n",
    "    \n",
    "#     x_repeat = tf.tile(x, [nsamples, 1, 1, 1])\n",
    "    \n",
    "#     mu_bar = tf.reduce_mean(samples, axis=0)\n",
    "#     sigma_bar = tfp.stats.covariance(samples, sample_axis=0, event_axis=-1)\n",
    "    \n",
    "#     return x_repeat, samples, mu_bar, sigma_bar\n",
    "    \n",
    "    \n",
    "def prepare_training(distr, y, optimizer_tuple, global_step, var_list):\n",
    "    cal_nll = tf.reduce_mean(-distr.log_prob(y), name=\"nll\")\n",
    "    optimizer = eval_method_from_tuple(tf.train, optimizer_tuple)\n",
    "    training_op = create_training_op(optimizer, cal_nll, global_step, var_list=var_list)\n",
    "    optimizer_variables = optimizer.variables()\n",
    "    sess.run(tf.variables_initializer(optimizer_variables))\n",
    "    sess.run(tf.variables_initializer([global_step]+var_list))\n",
    "    return cal_nll, training_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argo.core.hooks.LoggerHelper import LoggerHelper\n",
    "log_dir = \"/data1/calibration/try_last\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3 fine tune last layer\n",
    "cal_last_nll, last_training_op = prepare_training(distr, y, optimizer_tuple, cal_global_step, calibration_vars_for_3)\n",
    "\n",
    "smean = tf.linalg.trace(calibration_tril)/3.\n",
    "tensors_nodes = [cal_last_nll, smean]\n",
    "tensors_names = [\"nll\", \"calibration_scmean\"]\n",
    "logger = LoggerHelper(log_dir, \"t_scaling_last\", tensors_names, tensors_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP FOR CALIBRATION\n",
    "calibration_datasets = [TRAIN, VALIDATION]\n",
    "nepochs = 30\n",
    "\n",
    "sess_extra_args = logger.get_sess_run_args()\n",
    "logger.reset(sess)\n",
    "\n",
    "for i in range(nepochs):\n",
    "    for DATASET_STR in calibration_datasets:\n",
    "        make_one_epoch(last_training_op, sess_extra_args, DATASET_STR)\n",
    "    \n",
    "    logger.log(sess, i)\n",
    "    logger.reset(sess)\n",
    "    logger.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(calibration_tril)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING OP FOR SCALAR CALIBRATION\n",
    "cal_sc_bar_nll, sc_bar_training_op = prepare_training(distr_bar, y, optimizer_tuple, cal_global_step, [calibration_scalar_bar])\n",
    "tensors_nodes = [cal_sc_bar_nll, calibration_scalar_bar]\n",
    "tensors_names = [\"nll\", \"calibration_sc\"]\n",
    "logger = LoggerHelper(log_dir, \"t_scaling_sc_bar\", tensors_names, tensors_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP FOR CALIBRATION\n",
    "calibration_datasets = [TRAIN, VALIDATION]\n",
    "nepochs = 30\n",
    "\n",
    "sess_extra_args = logger.get_sess_run_args()\n",
    "logger.reset(sess)\n",
    "\n",
    "for i in range(nepochs):\n",
    "    for DATASET_STR in calibration_datasets:\n",
    "        make_one_epoch(sc_bar_training_op, sess_extra_args, DATASET_STR)\n",
    "    \n",
    "    logger.log(sess, i)\n",
    "    logger.reset(sess)\n",
    "    logger.plot()\n",
    "\n",
    "sess.run(calibration_scalar_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_repeat, mu_stack, sigma_stack = create_mu_bar_sigma_bar(x, distr, nsamples=3, scope=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_repeat, samples, mu_bar, sigma_bar = create_mu_bar_sigma_bar_samples(x, distr, nsamples=100, scope=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_repeat_np, samples_np, mu_bar_np, sigma_bar_np = sess.run([x_repeat, samples, mu_bar, sigma_bar], feed_dict={\n",
    "                                            ds_handle : datasets_handles[TRAIN_LOOP]\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_np[3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_repeat_np, mu_stack_np, sigma_stack_np = sess.run([x_repeat , mu_stack, sigma_stack], feed_dict={\n",
    "                                            ds_handle : datasets_handles[TRAIN_LOOP]\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(mu_stack_np[0], mu_stack_np[33]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_stack_np[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = distr.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_np, samples_np = sess.run([loc, samples], feed_dict={\n",
    "                                            ds_handle : datasets_handles[TRAIN_LOOP]\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_np.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = [\"loss\", \"nll\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0] = [0.4, 0.7]\n",
    "df.loc[1] = [0.4, 0.7]\n",
    "df.loc[2] = [0.2, 1.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "df.plot(subplots=True, sharex=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TRAINING OP FOR TRIL CALIBRATION\n",
    "cal_nll = tf.reduce_mean(-distr.log_prob(y), name=\"nll\")\n",
    "tril_optimizer = eval_method_from_tuple(tf.train, (optimizer_name, optimizer_kwargs))\n",
    "tril_training_op = create_training_op(tril_optimizer, cal_nll, cal_global_step, var_list=[calibration_tril_params])\n",
    "tril_optimizer_variables = tril_optimizer.variables()\n",
    "\n",
    "# variables of the optimizer need to be initialized\n",
    "sess.run(tf.variables_initializer(tril_optimizer_variables))\n",
    "sess.run(tf.variables_initializer([cal_global_step]))\n",
    "\n",
    "tensors_nodes = [cal_nll]\n",
    "tensors_names = [\"nll\", \"calibration_tril\"]\n",
    "\n",
    "logger = LoggerHelper(log_dir, \"temp_scaling_tril\", tensors_names, tensors_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP FOR TRIL CALIBRATION\n",
    "calibration_datasets = [TRAIN, VALIDATION]\n",
    "nepochs = 30\n",
    "\n",
    "sess_extra_args = logger.get_sess_run_args()\n",
    "logger.reset(sess)\n",
    "\n",
    "for i in range(nepochs):\n",
    "    for DATASET_STR in calibration_datasets:\n",
    "        make_one_epoch(tril_training_op, sess_extra_args, DATASET_STR)\n",
    "    \n",
    "    logger.log(sess, i)\n",
    "    logger.reset(sess)\n",
    "    logger.plot()\n",
    "\n",
    "sess.run(calibration_tril_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING OP FOR SCALAR CALIBRATION\n",
    "cal_sc_nll = tf.reduce_mean(-sc_distr.log_prob(y), name=\"nll\")\n",
    "sc_optimizer = eval_method_from_tuple(tf.train, (optimizer_name, optimizer_kwargs))\n",
    "sc_training_op = create_training_op(sc_optimizer, cal_sc_nll, cal_global_step, var_list=[calibration_scalar])\n",
    "sc_optimizer_variables = sc_optimizer.variables()\n",
    "\n",
    "sess.run(tf.variables_initializer(sc_optimizer_variables))\n",
    "sess.run(tf.variables_initializer(cal_global_step))\n",
    "sess.run(tf.variables_initializer([calibration_scalar]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_optimizer_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP FOR TRIL CALIBRATION\n",
    "calibration_datasets = [TRAIN, VALIDATION]\n",
    "nepochs = 1\n",
    "\n",
    "for i in range(nepochs):\n",
    "    for DATASET_STR in calibration_datasets:\n",
    "        make_one_epoch(sc_training_op, [cal_sc_nll], DATASET_STR)\n",
    "\n",
    "sess.run(calibration_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(calibration_tril_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY AT THE VERY END TO SAVE THE NETWORK\n",
    "path = \"verymeaningfulnameformycalibratednetwork.ckpt\"\n",
    "saver.save(sess, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
