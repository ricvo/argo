{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plt.plot(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1220 12:39:29.297142 140575383492416 deprecation_wrapper.py:119] From /data1/env/tf1.14.0/lib/python3.6/site-packages/sonnet/python/custom_getters/restore_initializer.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1220 12:39:30.091002 140575383492416 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1220 12:39:31.061160 140575383492416 deprecation_wrapper.py:119] From /home/deliaed/adversarial/core/argo/core/hooks/ArgoHook.py:25: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "W1220 12:39:31.213542 140575383492416 deprecation_wrapper.py:119] From /home/deliaed/adversarial/core/argo/core/hooks/CheckpointSaverHook.py:7: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\n",
      "\n",
      "W1220 12:39:31.214708 140575383492416 deprecation_wrapper.py:119] From /home/deliaed/adversarial/core/argo/core/hooks/CheckpointSaverHook.py:13: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0, \"/home/delia/adversarial\")\n",
    "import re\n",
    "# from adversarial.core.AdversarialModel import load_model\n",
    "# from adversarial.core.argo.core.ArgoLauncher import ArgoLauncher\n",
    "# from adversarial.core.transform.transform import load_network_with_dummy_x\n",
    "from core.AdversarialModel import load_model\n",
    "from core.argo.core.ArgoLauncher import ArgoLauncher\n",
    "from core.transform.transform import load_network_with_dummy_x\n",
    "from core.argo.core.TFDeepLearningModel import load_network\n",
    "from core.argo.core.utils.argo_utils import load_class\n",
    "\n",
    "# from adversarial.datasets.Dataset import Dataset\n",
    "from datasets.Dataset import Dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import collections\n",
    "\n",
    "pd_csv_kwargs = {\n",
    "    'sep' : '\\t'\n",
    "}\n",
    "\n",
    "def all_same(items):\n",
    "    return all(x == items[0] for x in items)\n",
    "\n",
    "def check_adv_matches(matches, base_dir):\n",
    "    trimmed_matches = [match.split(base_dir)[1].strip('/') for match in matches]\n",
    "    ds_strings, net_strings, transf_strings, attack_strings = zip(*[m.split(\"/\")[:4] for m in trimmed_matches])\n",
    "    if not all_same(ds_strings):\n",
    "        raise Exception(\"ds_strings are not all the same, check your regular expression, found: {:}\".format(set(ds_strings)))\n",
    "    if not all_same(attack_strings):\n",
    "        raise Exception(\"attack_string are not all the same, check your regular expression, found: {:}\".format(set(attack_strings)))\n",
    "    \n",
    "    net_transf_strings_grouped = collections.defaultdict(list)\n",
    "\n",
    "    for net_string, transf_string in zip(net_strings, transf_strings):\n",
    "        net_transf_strings_grouped[net_string].append(transf_string)\n",
    "    \n",
    "    return  list(set(ds_strings))[0], net_transf_strings_grouped, list(set(attack_strings))[0]\n",
    "\n",
    "\n",
    "# ds_dir = \"FashionMNIST-st0\" \n",
    "ds_dir = \"BTSC\"\n",
    "\n",
    "# base_dir = \"/data/thor_hd1/small_adv_new_experiments\"\n",
    "base_dir = \"/data1/small_contractive-J2_experiments/\"\n",
    "net_dir = \"FF-cCE-st0-stp*-r0\"\n",
    "# transf_dir = \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\"\n",
    "# transf_dir = \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128rSCR-n*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\"\n",
    "transf_dir = \"recVAE-*\"\n",
    "# transf_dir = \"recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.01_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\"\n",
    "\n",
    "attack_dir = \"advCW_n100*\"\n",
    "\n",
    "dataset_str=\"test\"\n",
    "# eps=0.1\n",
    "eps = 0.01\n",
    "# param_specs=[('d',), ('stp',)] #, ('ne', 'sm')]\n",
    "param_specs=[('d',), ('stp',) , ('ne', 'sm')]\n",
    "\n",
    "adv_folder=\"adversarial\"\n",
    "# outdirname=\"/data2/collected_adv_data\"\n",
    "outdirname=\"/data1/delia_experiments/collected_adv_data\"\n",
    "\n",
    "\n",
    "log_name = \"test-eps{:}\".format(eps)\n",
    "log_file = log_name+'.npy'\n",
    "dir_list = [base_dir, ds_dir, net_dir, transf_dir, attack_dir, adv_folder, log_file]\n",
    "matches = glob(os.path.join(*dir_list))\n",
    "if len(matches)==0:\n",
    "    raise ValueError(\"No file found matching the provided regexpr: `{:}`\".format(os.path.join(*dir_list)))\n",
    "\n",
    "ds_string, net_transf_strings, attack_string = check_adv_matches(matches, base_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'FF-cCE-st0-stp0.0-bs32-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-nC32o3,3_BN_P2s2_DOr0.3_C32o3,3_BN_P2s2_DOr0.3_C32o3,3_BN_P2s2_DOr0.3_C32o3,3_BN_P2s2_DOr0.3_D500_D62-cpS-aR-wix-bic0.1-wrLtwo0.001-brLtwo0.001-r0': ['recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.01_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0',\n",
       "              'recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.1_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0',\n",
       "              'recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.001_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0',\n",
       "              'recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm1.0_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0',\n",
       "              'recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.0_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0']})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_transf_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for transf_string in transf_strings:\n",
    "# # plot with fixed transf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_string = list(net_transf_strings.keys())[0]\n",
    "transf_strings = net_transf_strings[net_string]\n",
    "transf_string = transf_strings[0]\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "# loading the Adversarial Model conf\n",
    "conf_file = os.path.join(*[base_dir, ds_string, net_string, transf_string, attack_string, \"experiment.conf\"])\n",
    "# model, dataset = load_model(conf_file, model_class_base_path=\"core\")\n",
    "dataset_conf, model_parameters, config = ArgoLauncher.process_conf_file(conf_file)\n",
    "transform_tuple = model_parameters[\"transform\"]\n",
    "# loading the Adversarial Model conf\n",
    "dataset = Dataset.load_dataset(dataset_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.get_raw_labels(\"test\")[:1000]\n",
    "n, bins, patches = plt.hist(x=labels, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SESSION\n",
    "tf.set_random_seed(1000)\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from word_embedding.test.core.clustering.utils import riemannian_dist\n",
    "def rough_sqdist_numerics_diagzero(sqdistances):\n",
    "\n",
    "    np.fill_diagonal(sqdistances, 0.)\n",
    "    min_dist = np.min(sqdistances)\n",
    "    max_dist = np.max(sqdistances)\n",
    "\n",
    "    if min_dist < 0.:\n",
    "        sqdistances += -np.min(sqdistances)\n",
    "        np.fill_diagonal(sqdistances, 0.)\n",
    "\n",
    "    return sqdistances\n",
    "\n",
    "def check_sqdist_numerics(sqdistances):\n",
    "    min_dist = np.min(sqdistances)\n",
    "    max_dist = np.max(sqdistances)\n",
    "\n",
    "    all_ok = min_dist >= 0 or -min_dist < DISTABSTOL  # ((max_dist-min_dist)/max_dist - 1 < DISTRELTOL and -min_dist < DISTABSTOL)\n",
    "    assert all_ok, \"sqdistances are wrong, found: min {:} ,  max {:}\".format(min_dist, max_dist)\n",
    "\n",
    "    if min_dist < 0.:\n",
    "        sqdistances += -np.min(sqdistances)\n",
    "\n",
    "    return sqdistances\n",
    "\n",
    "\n",
    "def riemannian_dist(xp, xq, g_matrix):\n",
    "\n",
    "    Ixp = np.matmul(g_matrix, xp.T)\n",
    "    sqnormp = np.sum(xp * Ixp.T, axis=1)\n",
    "\n",
    "    Ixq = np.matmul(g_matrix, xq.T)\n",
    "    sqnormq = np.sum(xq * Ixq.T, axis=1)\n",
    "\n",
    "    sqdistances = sqnormp.reshape(-1, 1) + sqnormq.reshape(1, -1) - 2 * np.matmul(xp, Ixq)\n",
    "\n",
    "    # # temporarily remove this check put a rough compliance function\n",
    "    # sqdistances = check_sqdist_numerics_diagzero(sqdistances)\n",
    "    sqdistances = rough_sqdist_numerics_diagzero(sqdistances)\n",
    "\n",
    "    \n",
    "    distances = np.sqrt(sqdistances)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def nwwild(string, field_spec):\n",
    "    l = len(field_spec)\n",
    "    \n",
    "    if l==0 or l>2:\n",
    "        raise ValueError(\"Not implemented tuple length `{:}`, found field spec `{:}`\".format(l, field_spec))\n",
    "    \n",
    "    val = get_field(string, field_spec)\n",
    "    \n",
    "    if l==1:\n",
    "        fsplit = '-'+field_spec[0]\n",
    "    elif l==2:\n",
    "        fsplit ='_'+field_spec[1]\n",
    "    \n",
    "    ssplit = fsplit+val\n",
    "    \n",
    "    \n",
    "    if ssplit in string:\n",
    "        pre_stp, post_stp = string.split(ssplit)\n",
    "        return pre_stp+fsplit+'[W]'+post_stp\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def name_with_wildcards(strings, param_specs):\n",
    "    for ps in param_specs:\n",
    "        strings = list(map(partial(nwwild, field_spec=ps), strings))\n",
    "        strings = [s for s in strings if s is not None]\n",
    "#         import pdb; pdb.set_trace()\n",
    "    if not all_same(strings):\n",
    "        raise Exception(\"strings not all the same, found: ```{:}```\".format(set(strings)))\n",
    "    \n",
    "    return list(set(strings))[0]\n",
    "\n",
    "def get_param_vals(string, param_specs):\n",
    "    param_vals = []\n",
    "    for ps in param_specs:\n",
    "        pv = get_field(string, ps)\n",
    "        param_vals.append(pv)\n",
    "    return param_vals\n",
    "\n",
    "def get_field(string, field_spec):\n",
    "    l = len(field_spec)\n",
    "    \n",
    "    if l==0 or l>2:\n",
    "        raise ValueError(\"Not implemented tuple length `{:}`, found field spec `{:}`\".format(l, field_spec))\n",
    "    \n",
    "    m = re.search('(-|^)' + field_spec[0] + '([\\._A-Za-z0-9\\,]+)' + '(-|$)', string)\n",
    "    if m is None:\n",
    "        ss1 = '0'\n",
    "    else:\n",
    "        ss1 = m.group(2)\n",
    "    \n",
    "    if l==1:\n",
    "        return ss1\n",
    "    \n",
    "    m = re.search('(_|^)' + field_spec[1] + '([\\.A-Za-z0-9\\,]+)' + '(_|$)', ss1)\n",
    "\n",
    "    if m is None:\n",
    "        ss2 = '0'\n",
    "    else:\n",
    "        ss2 = m.group(2)\n",
    "    \n",
    "    return ss2\n",
    "\n",
    "def spec_name(param_spec):\n",
    "    return \"_\".join(param_spec)\n",
    "\n",
    "def extract_one_point_info(base_dir, ds_string, net_string, transf_string, attack_string, sess_config, dataset_str, eps):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session(config = sess_config)\n",
    "    \n",
    "    # loading the Adversarial Model conf\n",
    "    conf_file = os.path.join(*[base_dir, ds_string, net_string, transf_string, attack_string, \"experiment.conf\"])\n",
    "    # model, dataset = load_model(conf_file, model_class_base_path=\"core\")\n",
    "    dataset_conf, model_parameters, config = ArgoLauncher.process_conf_file(conf_file)\n",
    "    transform_tuple = model_parameters[\"transform\"]\n",
    "    # loading the Adversarial Model conf\n",
    "\n",
    "    dataset = Dataset.load_dataset(dataset_conf)\n",
    "    x_adv = np.load(os.path.join(*[base_dir, ds_string, net_string, transf_string, attack_string, \"adversarial\", dataset_str+\"-eps{:}.npy\".format(eps)]))\n",
    "    x_clean = dataset.get_raw_elements(dataset_str)[:1000]\n",
    "    # # this checks the norms are what they should be\n",
    "    # # np.linalg.norm(x_clean.reshape(-1,784)-x_adv.reshape(-1,784), ord=np.inf, axis=1)\n",
    "    # loading the transformation\n",
    "    transform_name, transform_kwargs = transform_tuple\n",
    "    is_training=False\n",
    "    transf_conf_file = transform_kwargs['conf_file']\n",
    "    global_step = transform_kwargs['global_step']\n",
    "    x_shape = (None,) + dataset.x_shape\n",
    "\n",
    "    x_ph = tf.placeholder(tf.float32, shape=x_shape, name='input')\n",
    "    \n",
    "    t_network = load_network_with_dummy_x(x_ph,\n",
    "                                        sess,\n",
    "                                        is_training=is_training,\n",
    "                                        conf_file=transf_conf_file,\n",
    "                                        global_step=global_step,\n",
    "                                        base_path=\"vae.core\")\n",
    "    # loading the transformation\n",
    "\n",
    "    encoder_module = t_network.encoder_module\n",
    "    decoder_module = t_network.decoder_module\n",
    "    \n",
    "    enc = encoder_module(x_ph)\n",
    "    \n",
    "    if transform_name=='vae':\n",
    "        mu = enc.mean()\n",
    "    elif transform_name=='ae':\n",
    "        mu = enc\n",
    "    else:\n",
    "        raise Exception(\"not supported\")\n",
    "    \n",
    "    mu_np = sess.run(mu, feed_dict={x_ph:x_clean})\n",
    "    mu_star_np = sess.run(mu, feed_dict={x_ph:x_adv})\n",
    "    norm_np = np.linalg.norm(mu_np, axis=1)\n",
    "    norm_star_np = np.linalg.norm(mu_star_np, axis=1)\n",
    "    adiff = norm_star_np - norm_np\n",
    "    costh = np.sum(mu_np * mu_star_np, axis=1)/(norm_np*norm_star_np)\n",
    "    \n",
    "    onevec = np.ones((1, mu_np.shape[1]))\n",
    "    norm_onevec = np.linalg.norm(onevec)\n",
    "    costh_mu = np.sum(mu_np * onevec, axis=1)/(norm_np*norm_onevec)\n",
    "    costh_mu_adv = np.sum(mu_star_np * onevec, axis=1)/(norm_star_np*norm_onevec)\n",
    "    nratio = norm_star_np/norm_np\n",
    "    \n",
    "    ldim = mu_np.shape[1]\n",
    "    euclidean_dist = partial(riemannian_dist, g_matrix=np.eye(ldim))\n",
    "    \n",
    "    distances_mu_np = euclidean_dist(mu_np, mu_np)\n",
    "    distances_mu_star_np = euclidean_dist(mu_star_np, mu_star_np)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    diameter_mu_set = np.max(distances_mu_np)\n",
    "    diameter_mu_star_set = np.max(distances_mu_star_np)\n",
    "\n",
    "# exclude the diagonal in case you want to do average of distances\n",
    "#     mask_out_of_diag = np.ones(distances_mu_np.shape, dtype=bool)\n",
    "#     np.fill_diagonal(mask_out_of_diag, 0)\n",
    "#     avgdist_mu_set = np.mean(distances_mu_np[mask_out_of_diag])\n",
    "#     avgdist_mu_star_set = np.mean(distances_mu_star_np[mask_out_of_diag])\n",
    "    \n",
    "    centroid_mu_set = np.mean(mu_np, axis=0)\n",
    "    centroid_mu_star_set = np.mean(mu_star_np, axis=0)\n",
    "    \n",
    "    cdists_set = euclidean_dist(mu_np, centroid_mu_set.reshape(1, -1))\n",
    "    cdists_adv_set = euclidean_dist(mu_star_np, centroid_mu_star_set.reshape(1, -1))\n",
    "    \n",
    "    ndiffs_np = np.linalg.norm(mu_star_np-mu_np, axis=1)\n",
    "    ndiff_avg = np.mean(ndiffs_np)\n",
    "    ndiff_std = np.std(ndiffs_np)\n",
    "    \n",
    "    adiff_avg = np.mean(adiff)\n",
    "    adiff_std = np.std(adiff)\n",
    "    nratio_avg = np.mean(nratio)\n",
    "    nratio_std = np.std(nratio)\n",
    "    costh_avg = np.mean(costh)\n",
    "    costh_std = np.std(costh)\n",
    "    costh_mu_avg = np.mean(costh_mu)\n",
    "    costh_mu_std = np.std(costh_mu)\n",
    "    costh_mu_adv_avg = np.mean(costh_mu_adv)\n",
    "    costh_mu_adv_std = np.std(costh_mu_adv)\n",
    "\n",
    "    norms_avg = np.mean(norm_np)\n",
    "    norms_std = np.std(norm_np)\n",
    "    \n",
    "    norms_adv_avg = np.mean(norm_star_np)\n",
    "    norms_adv_std = np.std(norm_star_np)\n",
    "\n",
    "    cdists_set_mean = np.mean(cdists_set)\n",
    "    cdists_set_std = np.std(cdists_set)\n",
    "    cdists_adv_set_mean = np.mean(cdists_adv_set)\n",
    "    cdists_adv_set_std = np.std(cdists_adv_set)\n",
    "    \n",
    "    sess.close()\n",
    "#     tf.reset_default_graph()\n",
    "    return [adiff_avg, adiff_std,\n",
    "            norms_avg, norms_std,\n",
    "            norms_adv_avg, norms_adv_std,\n",
    "            ndiff_avg, ndiff_std,\n",
    "            nratio_avg, nratio_std,\n",
    "            costh_avg, costh_std,\n",
    "            costh_mu_avg, costh_mu_std,\n",
    "            costh_mu_adv_avg, costh_mu_adv_std,\n",
    "            cdists_set_mean, cdists_set_std,\n",
    "            cdists_adv_set_mean, cdists_adv_set_std,\n",
    "            diameter_mu_set,\n",
    "            diameter_mu_star_set,\n",
    "           ]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'stp', 'ne_sm']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_names = list(map(spec_name, param_specs))\n",
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = transf_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.01'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_field(string, ('ne', 'sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0.1', '0.01']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_param_vals(string, param_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_sm' in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search('(-|^)' + 'ne' + '[\\._A-Za-z0-9\\,]+'+ '([\\_])'+ 'sm' + '(-|$)', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.01_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\n",
      "recVAE-cELBO_b1.0_wuW5-s10-d[W]-st1-stp0.1-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.01_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\n",
      "recVAE-cELBO_b1.0_wuW5-s10-d0-st1-stp[W]-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm0.01_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\n"
     ]
    }
   ],
   "source": [
    "print(string)\n",
    "print(nwwild(string, param_specs[0]))\n",
    "print(nwwild(string, param_specs[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb; pdb.set_trace()\n",
    "outputname = name_with_wildcards(transf_strings, param_specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recVAE-cELBO_b1.0_wuW5-s10-d[W]-st1-stp[W]-bs128-trA_lr0.0001_bo0.9_bt0.999-cGN100.0-ne_CNo32,32k3,3s2,2_GDd128rSCR_n2_sm[W]_sc0.0-nd_D1568_BR_CNTo32,1k3,3s2,2_GDdmc0.01-cpS-aR-wix-bic0.1-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_stats_over_transf(ds_string, net_string, transf_strings, attack_string, param_specs, dataset_str, eps, outdirname):\n",
    "\n",
    "    param_names = list(map(spec_name, param_specs))\n",
    "    all_fields = param_names + [\"adiff_avg\", \"adiff_std\",\n",
    "                                \"norm_avg\", \"norm_std\",\n",
    "                                \"norm_adv_avg\", \"norm_adv_std\",\n",
    "                                \"ndiff_avg\", \"ndiff_std\",\n",
    "                                \"nratio_avg\", \"nratio_std\",\n",
    "                                \"costh_avg\", \"costh_std\",\n",
    "                                \"costh_mu_avg\", \"costh_mu_std\",\n",
    "                                \"costh_mu_adv_avg\", \"costh_mu_adv_std\",\n",
    "                                \"cdist_avg\", \"cdist_std\",\n",
    "                                \"cdist_adv_avg\", \"cdist_adv_std\",\n",
    "                                \"diameter_mu\",\n",
    "                                \"diameter_mu_adv\",]\n",
    "    data = []\n",
    "    for transf_string in transf_strings:\n",
    "        param_vals = get_param_vals(transf_string, param_specs)\n",
    "\n",
    "        info_values_list = extract_one_point_info(base_dir, ds_string, net_string, transf_string, attack_string, sess_config, dataset_str, eps)\n",
    "        \n",
    "        data.append(param_vals + info_values_list)\n",
    "\n",
    "    sorted_data = sorted(data)\n",
    "\n",
    "    df = pd.DataFrame(sorted_data, columns=all_fields)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    outputname = name_with_wildcards(transf_strings, param_specs)\n",
    "\n",
    "    out_dir_list = [outdirname, ds_string, attack_string, net_string]\n",
    "    outpath = os.path.join(*out_dir_list)\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "    outpath = os.path.join(outpath,  outputname+'-ds{:}-eps{:}.csv'.format(dataset_str, eps))\n",
    "    df.to_csv(outpath, index=False, float_format='%.6g', **pd_csv_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats_per_dataset(ds_dir, net_dir, transf_dir, attack_dir, eps, base_dir, outdirname):\n",
    "    \n",
    "    dataset_str=\"test\"\n",
    "#     param_specs=[('d',), ('stp',)] #, ('ne', 'sm')]\n",
    "    param_specs=[('d',), ('stp',), ('ne', 'sm')]\n",
    "\n",
    "    adv_folder=\"adversarial\"\n",
    "\n",
    "    log_name = \"test-eps{:}\".format(eps)\n",
    "    log_file = log_name+'.npy'\n",
    "    dir_list = [base_dir, ds_dir, net_dir, transf_dir, attack_dir, adv_folder, log_file]\n",
    "    matches = glob(os.path.join(*dir_list))\n",
    "    if len(matches)==0:\n",
    "        raise ValueError(\"No file found matching the provided regexpr: `{:}`\".format(os.path.join(*dir_list)))\n",
    "\n",
    "    ds_string, net_transf_strings_grouped, attack_string = check_adv_matches(matches, base_dir)\n",
    "\n",
    "    # for net_string in net_strings:\n",
    "    # # plot with fixed net\n",
    "    for net_string in net_transf_strings_grouped:\n",
    "        transf_strings = net_transf_strings_grouped[net_string]\n",
    "        collect_stats_over_transf(ds_string, net_string, transf_strings, attack_string, param_specs, dataset_str, eps, outdirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No file found matching the provided regexpr: `/data1/small_contractive-J2_experiments/BTSC/FF-cCE-st0-stp*-r0/recVAE-*/advCW_n100*/adversarial/test-eps0.1.npy`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-99c6864f3343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnet_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilons\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet_dirs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_dir\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mcollect_stats_per_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# ds_dir = ds_dirs[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-eef813145b1c>\u001b[0m in \u001b[0;36mcollect_stats_per_dataset\u001b[0;34m(ds_dir, net_dir, transf_dir, attack_dir, eps, base_dir, outdirname)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdir_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No file found matching the provided regexpr: `{:}`\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdir_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mds_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_transf_strings_grouped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_adv_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No file found matching the provided regexpr: `/data1/small_contractive-J2_experiments/BTSC/FF-cCE-st0-stp*-r0/recVAE-*/advCW_n100*/adversarial/test-eps0.1.npy`"
     ]
    }
   ],
   "source": [
    "# base_dir = \"/data1/small_adv_new_experiments\"\n",
    "# base_dir = \"/data/wonderwoman_hd1/small_adv_new_experiments\"\n",
    "base_dir = \"/data1/small_contractive-J2_experiments/\"\n",
    "# ds_dirs = [\"MNIST-c-st0\", \"FashionMNIST-st0\"] #\"SVHN-st0\"]\n",
    "# ds_dirs = [\"MNIST-c-st0\"]\n",
    "ds_dirs = ['BTSC']\n",
    "#\"BTSC\", \n",
    "\n",
    "# outdirname=\"/data2/collected_adv_data\"\n",
    "outdirname=\"/data1/delia_experiments/collected_adv_data\"\n",
    "\n",
    "\n",
    "# net_dirs_dict = {\n",
    "    \n",
    "#     \"FashionMNIST-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\", [0.1, 0.2, 0.3]),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", [0.1, 0.2, 0.3]),\n",
    "#     ],\n",
    "    \n",
    "#     \"MNIST-c-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\", [0.1, 0.2, 0.3]),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cV10*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", [0.1, 0.2, 0.3]),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cGN100.0*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", [0.1, 0.2, 0.3]),\n",
    "#     ],\n",
    "    \n",
    "#     \"SVHN-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\", 0.03),\n",
    "#         (\"FF-cCE-st0-*-wrLtwo0.001-brLtwo0.001-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.03)\n",
    "#     ],\n",
    "    \n",
    "#     \"BTSC\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.02)\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "# net_dirs_dict = {\n",
    "    \n",
    "#     \"MNIST-c-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\", [0.1, 0.2, 0.3]),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cV10*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", [0.1, 0.2, 0.3]),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cGN100.0*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", [0.1, 0.2, 0.3]),\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "net_dirs_dict = {\n",
    "    \n",
    "    \"BTSC\" : [\n",
    "        (\"FF-cCE-st0-stp*-r0\", \"recVAE-*\", 0.01)\n",
    "    ],\n",
    "}\n",
    "\n",
    "attack_dir = \"advCW_n100*\"\n",
    "\n",
    "# for ds_dir in ds_dirs:\n",
    "#     for net_dir, transf_dir, epsilons in net_dirs_dict[ds_dir]:\n",
    "#         for eps in epsilons:\n",
    "#             collect_stats_per_dataset(ds_dir, net_dir, transf_dir, attack_dir, eps, base_dir, outdirname)\n",
    "\n",
    "ds_dir = ds_dirs[0]\n",
    "net_dir, transf_dir, epsilons = net_dirs_dict[ds_dir][0]\n",
    "collect_stats_per_dataset(ds_dir, net_dir, transf_dir, attack_dir, epsilons, base_dir, outdirname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_curves(df, x, y, groupfield, pre_label):\n",
    "    garr = [(label, df) for label, df in df.groupby(groupfield)]\n",
    "    for groupval, df in garr:\n",
    "        plabel = pre_label+'-'+groupfield+\"{:}\".format(groupval)\n",
    "        xdata = df[x]\n",
    "        \n",
    "        pkwargs = {\n",
    "            'alpha' : ALPHA,\n",
    "            'label' : plabel\n",
    "        }\n",
    "        \n",
    "        if y+'_avg' in df.columns and y+'_std' in df.columns:  \n",
    "            ydata = df[y+'_avg']\n",
    "            yerr = df[y+'_std']\n",
    "            plt.errorbar(xdata, ydata, yerr=yerr, **pkwargs)\n",
    "        else:\n",
    "            ydata = df[y]\n",
    "            plt.plot(xdata, ydata, **pkwargs)\n",
    "\n",
    "\n",
    "nice_yaxis_names = {\n",
    "    'nratio' : '$|\\mu^*|/|\\mu|$',\n",
    "    'costh' : r'$\\cos \\, \\theta$',\n",
    "    'costh_mu' : r'$\\cos \\, \\theta$',\n",
    "    'costh_mu_adv' : r'$\\cos \\, \\theta$',\n",
    "    'adiff' : '$|\\mu^*| - |\\mu|$',\n",
    "    'ndiff' : '$|\\mu^* - \\mu|$',\n",
    "    'norm' : '$|\\mu|$',\n",
    "    'norm_adv' : '$|\\mu^*|$',\n",
    "    'cdist' : 'centroid dist',\n",
    "    'cdist_adv' : 'centroid dist',\n",
    "    'diameter_mu' : 'diameter',\n",
    "    'diameter_mu_adv' : 'diameter',\n",
    "}\n",
    "\n",
    "ALPHA = 0.6\n",
    "\n",
    "def make_ds_plot(ds_dir, csv_dirs_dict, y, eps, plot_dir):\n",
    "    x = 'stp'\n",
    "    nice_ds_name = ds_dir.split(\"-\")[0]\n",
    "    for net_dir, transf_dir, sm in csv_dirs_dict[ds_dir]:\n",
    "        transf_csv = transf_dir+'*.csv'\n",
    "\n",
    "        dir_list = [base_csv_dir, ds_dir, attack_dir, net_dir, transf_csv]\n",
    "        matches = list(set(glob(os.path.join(*dir_list))))\n",
    "\n",
    "        # print(matches)\n",
    "        groupfield = 'd'\n",
    "        plt.title(nice_ds_name+' eps={:}'.format(eps))\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(nice_yaxis_names[y])\n",
    "        for match in sorted(matches):\n",
    "            transf_string = os.path.splitext(match.split(\"/\")[-1])[0]\n",
    "            recv = get_field(transf_string, ('rec',))\n",
    "            dsv = get_field(transf_string, ('ds',))\n",
    "            epsv = get_field(transf_string, ('eps',))\n",
    "            if not np.isclose(eps, float(epsv)):\n",
    "                continue\n",
    "            df = pd.read_csv(match, **pd_csv_kwargs)\n",
    "#             import pdb; pdb.set_trace()\n",
    "            plot_grouped_curves(df, x, y, groupfield, recv)\n",
    "    \n",
    "    fields = [nice_ds_name, x, y, 'eps{:.2f}'.format(eps)]\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(os.path.join(plot_dir, '-'.join(fields)+'.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_plot_in_place(ds_dir, csv_dirs_dict, y, eps, plot_dir):\n",
    "#     fig = plt.figure()\n",
    "    x = 'stp'\n",
    "    nice_ds_name = ds_dir.split(\"-\")[0]\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for net_dir, transf_dir, sm in csv_dirs_dict[ds_dir]:\n",
    "        transf_csv = transf_dir+'*.csv'\n",
    "\n",
    "        dir_list = [base_csv_dir, ds_dir, attack_dir, net_dir, transf_csv]\n",
    "        matches = list(set(glob(os.path.join(*dir_list))))\n",
    "\n",
    "        # print(matches)\n",
    "        groupfield = 'd'\n",
    "        plt.title(nice_ds_name+' eps={:}'.format(eps))\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(nice_yaxis_names[y])\n",
    "        for match in sorted(matches): \n",
    "#             print(match)\n",
    "            transf_string = os.path.splitext(match.split(\"/\")[-1])[0]\n",
    "            recv = get_field(transf_string, ('rec',))\n",
    "            dsv = get_field(transf_string, ('ds',))\n",
    "            epsv = get_field(transf_string, ('eps',))\n",
    "#             import pdb; pdb.set_trace()\n",
    "            if not np.isclose(eps, float(epsv)):\n",
    "                continue\n",
    "            df = pd.read_csv(match, **pd_csv_kwargs)\n",
    "#             import pdb; pdb.set_trace()\n",
    "            plot_grouped_curves(df, x, y, groupfield, recv)\n",
    "    \n",
    "    fields = [nice_ds_name, x, y, 'eps{:.2f}'.format(eps)]\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(os.path.join(plot_dir, '-'.join(fields)+'.png'))\n",
    "    print(os.path.join(plot_dir, '-'.join(fields)+'.png'))\n",
    "    plt.show()\n",
    "#     plt.clf()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'savefig.dpi': '300'})\n",
    "\n",
    "fontsize=30\n",
    "fonttitlesize=34\n",
    "fontaxeslabelsize=30\n",
    "fontlegendsize=24\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': fontsize})\n",
    "# matplotlib.rcParams.update({'font.weight': 'bold'})\n",
    "# matplotlib.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
    "\n",
    "matplotlib.rcParams.update({'legend.frameon': False})\n",
    "matplotlib.rcParams.update({'legend.fontsize': fontlegendsize})\n",
    "matplotlib.rcParams.update({'legend.borderaxespad': 1.0})\n",
    "\n",
    "matplotlib.rcParams.update({'lines.linewidth': 4.0})\n",
    "matplotlib.rcParams.update({'lines.markersize': 9})\n",
    "matplotlib.rcParams.update({'lines.marker': 'o'})\n",
    "\n",
    "matplotlib.rcParams.update({'errorbar.capsize': 7})\n",
    "# matplotlib.rcParams.update({'errorbar.capthick': 3})\n",
    "\n",
    "matplotlib.rcParams.update({'axes.titlesize': fonttitlesize})\n",
    "# matplotlib.rcParams.update({'axes.titleweight': 'bold'})\n",
    "matplotlib.rcParams.update({'axes.labelsize': fontaxeslabelsize})\n",
    "# matplotlib.rcParams.update({'axes.labelweight': 'bold'})\n",
    "\n",
    "# matplotlib.rcParams.update({'axes.labelpad': 16.0})\n",
    "# matplotlib.rcParams.update({'xtick.major.pad': 10.0})\n",
    "# matplotlib.rcParams.update({'ytick.major.pad': 5.0})\n",
    "\n",
    "figsize = matplotlib.rcParams['figure.figsize']\n",
    "figsize[0] = 6.4 * 2\n",
    "figsize[1] = 4.8 * 2\n",
    "\n",
    "\n",
    "base_csv_dir = outdirname\n",
    "# base_plot_dir = \"/data2/collected_adv_data_plots\"\n",
    "base_plot_dir = \"/data1/delia_experiments/collected_adv_data_plots\"\n",
    "\n",
    "# ds_dirs = [\"MNIST-c-st0\", \"FashionMNIST-st0\"]\n",
    "# ds_dirs = [\"MNIST-c-st0\"]\n",
    "ds_dirs = ['BTSC']\n",
    "# csv_dirs_dict = {\n",
    "    \n",
    "#     \"FashionMNIST-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\"),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-*\"),\n",
    "#     ],\n",
    "    \n",
    "#     \"MNIST-c-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\"),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cV10*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-*\"),\n",
    "# #         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cGN100.0*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.1),\n",
    "#     ],\n",
    "    \n",
    "# #     \"SVHN-st0\" : [\n",
    "# #         (\"FF-cCE-st0-*-wrLtwo0.001-brLtwo0.001-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.015)\n",
    "# #     ],\n",
    "    \n",
    "# #     \"BTSC\" : [\n",
    "# #         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.02)\n",
    "# #     ],\n",
    "# }\n",
    "# csv_dirs_dict = {\n",
    "    \n",
    "#     \"MNIST-c-st0\" : [\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recAE-cL2-d*-st1-stp*-trA_lr0.0001_*_D128-nd*-wrLtwo0.01-brLtwo0.01-*\"),\n",
    "#         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cV10*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-*\"),\n",
    "# #         (\"FF-cCE-st0-stp*-r0\", \"recVAE-cELBO_b1.0_wuW5-s10-d*-st1-stp*-trA_lr0.0001_*-cGN100.0*_GDd128-nd*-wrLtwo0.01-brLtwo0.01-r0-sh0-sx0-zsc1.0\", 0.1),\n",
    "#     ],\n",
    "    \n",
    "# }\n",
    "\n",
    "csv_dirs_dict = {\n",
    "    \"BTSC\" : [\n",
    "        (\"FF-cCE-st0-stp*-r0\", \"recVAE-*\", 0.01)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# os.makedirs(base_plot_dir, exist_ok=True)\n",
    "# for ds_dir in ds_dirs:\n",
    "#     for y in ['nratio', 'costh', 'costh_mu', 'costh_mu_adv', 'adiff', 'ndiff', 'norm', 'norm_adv', 'cdist', 'cdist_adv', 'diameter_mu', 'diameter_mu_adv']:\n",
    "# #         for eps in [0.1, 0.2 ,0.3]:\n",
    "#         make_ds_plot_in_place(ds_dir, csv_dirs_dict, y, 0.01, base_plot_dir)\n",
    "    \n",
    "os.makedirs(base_plot_dir, exist_ok=True)\n",
    "for ds_dir in ds_dirs:\n",
    "    for y in ['nratio']:\n",
    "        make_ds_plot(ds_dir, csv_dirs_dict, y, 0.01, base_plot_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for label, df in df.groupby(groupfield):\n",
    "    \n",
    "#     plt.errorbar(xdata, ydata, yerr=yerr, label='VAE-'groupfield+\"{:}\".format(label))\n",
    "    \n",
    "\n",
    "# plt.tight_layout()\n",
    "# plotfilename = self._plotfilename\n",
    "# if suffix is not None:\n",
    "#     plotfilename+=suffix\n",
    "# plt.savefig(plotfilename+\".png\")\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6b1d2265cb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "garr = [(label, df) for label, df in df.groupby(groupfield)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garr[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 'adiff'\n",
    "# y = 'costh'\n",
    "y = 'nratio'\n",
    "\n",
    "\n",
    "for groupval, df in garr:\n",
    "    xdata = df['stp']\n",
    "    ydata = df[y+'_mu']\n",
    "    yerr = df[y+'_std']\n",
    "    plt.errorbar(xdata, ydata, yerr=yerr, label='VAE-'+groupfield+\"{:}\".format(groupval))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._transform_feedable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.test_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(out, feed_dict={x_placeholder:x_clean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(model._logits, feed_dict={model.x: x_clean})\n",
    "sess.run(model._transform_module.enc.sample(), feed_dict={model.x: x_clean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_lpsch_avg = \"\"\n",
    "field_lpsch_std = \"\"\n",
    "field_costh_avg = \"\"\n",
    "field_costh_std = \"\"\n",
    "\n",
    "all_fields = [param_name, 'epoch', field_train, field_val, field_test]\n",
    "\n",
    "data = []\n",
    "for match in matches:\n",
    "    x = get_ds_field_value(match, pre_ds_dir, post_ds_dir)\n",
    "    epmax, y_val = read_max_value(match, field_val, 'epoch')\n",
    "    y_train = read_value(match, 'epoch', epmax, field_train)\n",
    "    y_test = read_value(match, 'epoch', epmax, field_test)\n",
    "    data.append((tryfloat(x), epmax, y_train, y_val, y_test))\n",
    "\n",
    "sorted_data = sorted(data, key=key_sort)\n",
    "\n",
    "df = pd.DataFrame(sorted_data, columns=all_fields)\n",
    "\n",
    "outpath = os.path.join(outdirname, pre_ds_dir+'W'+post_ds_dir, net_string)\n",
    "os.makedirs(outpath, exist_ok=True)\n",
    "outpath = os.path.join(outpath, \"collected_\"+log_file)\n",
    "df.to_csv(outpath, index=False, float_format='%.6g', **pd_csv_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
