
# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "MNIST",
"binary" : 0,
"flip_fraction" : 0.,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{
"model" : ["AE.AutoEncoder"],

"stochastic" : [1],
"stochastic_noise_param" : [0, 0.01], #0, ,0.00001,0.001,0.1,1
"denoising" : [0, 1],

"optimizer": [("AdamOptimizer", {"learning_rate" : 0.001,	
	     			 "beta1" : 0.9,
                      		 "beta2" : 0.999})],

#"grad_clipping" : (None, {}),
"grad_clipping" : ("clip_by_norm", {"value" : 1}),


"cost_function": [("L2", {})
		  ],

"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [
        {
        "encoder" : [
            ("BatchFlatten", {}, 0),
            ("Linear", {"output_size" : 200}, 1),
            ("Linear", {"output_size" : 200}, 1),
	    ("Linear", {"output_size" : size_h,
	    	        "contractive_regularizer" : ("standard_contractive_regularizer",
                                                     {"scale" : scale,
                                                      "norm" : 2})
			}, 1,
			["ContractiveRegularizer"]
			),
            ],
        "decoder" : [
            ("Linear", {"output_size" : 200}, 1),
            ("Linear", {"output_size" : 200}, 1),
            ("LogitNormalDiagonalPlusMinusOne", {"module_tuple" : ("LinearWN", {"use_weight_norm" : False}),
	    			                 "clip_value" : 0.0001}, 0)
	    #("GaussianDiagonalZeroOne", {"module_tuple" : ("LinearWN", {"use_weight_norm" : False})}, 0)
            ]
        }
	for size_h in [64, 128] for scale in [0, 0.1, 0.5]
	],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],


"weights_init" : [("contrib.layers.xavier_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],
#	       ("constant_initializer", {'value' : 0.01})],

"activation" : ["relu"],
"epochs" : 300,

},
{
"check_ops" : 0,

# choose steps or epochs
"time_reference" : "epochs",

# how often to log stats (txt) and save summaries (related to the stats)
"stats_period" : 1,

# choose to save the model every n steps
"save_model" : 1,

"save_model_period" : 10,

# how often to save other specific summaries
"save_summaries" : 0,
"save_summaries_period" : 10,

# skips the first k stats for the plot
"plot_offset" : 1,

"GradientsHook" : {"period" : 10},


"ImagesInputHook" : {"period" : 100,
                    "how_many" : 18,
                    "n_images_columns" : 6},

"ImagesReconstructHook" : {"period" : 100,
                             "n_images_columns" : 6,
                             "images_indexes" : {
                                                 "train" : [0,100,200,300,400,500],
                                                 "validation" : [0,100,200,300,400,500],
                                                 },
                            },


"TwoDimPCALatentVariablesHook" : {"period" : 50,
                                  },

"PCALatentVariablesHook" : {"period" : 50,
                             },


#"LogpImportanceSamplingHook" : {"period" : 100,
#		       	        "n_samples" : [1,10],
#		     		"batch_size" : 1000,
#				"repetitions" : 3
#                       	       },


"dirName" : "/data1/luigi",


"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {0,1,2,3},
            "cores_per_GPU" : 2,
	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

# NOT USED AFTER THIS 

"log_latent_vars_model" : 0,
"n_epoch_log_latent_vars_model" : 25,
"log_latent_vars_model_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_latent_vars_model_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs x_train[subset], x_validate[subset], 2 nodes "Distribution" of the visible
"log_visible_vars_model" : 0,
"n_epoch_log_visible_vars_model" : 25,
"log_visible_vars_model_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_visible_vars_model_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs x_train[subset], x_validate[subset], vae.regenerate()
"log_repeated_reconstructions" : 0,
"n_epoch_log_repeated_reconstruction" : 25,
"n_repeated_reconstructions" : 100,
"log_repeated_reconstruction_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_repeated_reconstruction_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs x_train, x_validate, vae.encode()
"log_latent_vars_corr" : 0,
"n_epoch_log_latent_vars_corr" : 100,


"log_estimate_function" : 0,

}
]
