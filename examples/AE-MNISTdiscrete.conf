# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "MNIST",
"binary" : 1,
"vect" : False,
"stochastic" : 1,
},
{
"model" : "AE.AutoEncoder",

#"synthetic" : 0,
"stochastic" : 0,
"stochastic_noise_param" : [0],
"denoising" : [0],
#"obs" : [0],

"warm_up_method" : [("warm_up", {"epoch" : 10}),
                     None],

"epsilon_z" : [0, 0.01, 0.1],
"epsilon_x" : [0, 0.01, 0.1], #, 0.01, 0],

"optimizer": [("AdamOptimizer", {"learning_rate" : 0.001,
					             "beta1": 0.9,
                      			 "beta2":0.999}),
              ("AdamOptimizer", {"learning_rate" : 0.0001,
					             "beta1": 0.9,
                      			 "beta2":0.999})
		      ],

#"cost_function": [{"cost" : 0}, #KL divergence (standard VAE bound)
#		  {"cost" : 1, "K" : 10}, #KL divergence with IS (from Variational with Monte Carlo objectives)
#		  {"cost" : 1, "K" : 20}, #KL divergence with IS (from Variational with Monte Carlo objectives)
#		  {"cost" : 2, "alpha" : 0.9, "K" : 10}, #Renyi divergence with IS from WS paper, which generalizes the Renyi paper
#		  {"cost" : 2, "alpha" : 0.9, "K" : 20} #Renyi divergence with IS from WS paper, which generalizes the Renyi paper
# 		  ],

"grad_clipping" : ("clip_by_norm", {"value" : 5}), # NB not implemented in the filename

"cost_function": [("L2", {})
		 ],

#"samples" : [1, 5, 10],
#"imp_samples" : [200],
"bit_flip" : [0],
"drop_out" : [0],
"latent_noise" : [0],

"batch_size_train" : [128],
"batch_size_eval" : [1000],


"network_architecture" : [
     {
       "encoder" : [
            ("BatchFlatten", {}, 0),
            ("Linear", {"output_size" : 200}, 1),
            ("Linear", {"output_size" : 100}, 1),
	    ("Linear", {"output_size" : 2}, 1)],
         	    
        "decoder" : [
            ("Linear", {"output_size" : 100}, 1),
            ("Linear", {"output_size" : 200}, 1),
     	    ("Bernoulli", {},  0)]
     }
		],


"weights_reg" : [None],
"bias_reg" : [None],

"weights_init" : [("contrib.layers.xavier_initializer",{})],  # !!! truncated normal
"bias_init" :    [("constant_initializer", {'value' : 0.1})],
#	         ("constant_initializer", {'value' : 0.01})],

"activation" : ["elu", "relu"],
"epochs" : 500,

},
{
"check_ops" : 0,

# choose steps or epochs
"time_reference" : "epochs",

# how often to log stats (txt) and save summaries (related to the stats)
"stats_period" : 1,

# choose to save the model every n steps
"save_model" : 1,
"save_model_period" : 10,

# how often to save other specific summaries
"save_summaries" : 1,
"save_summaries_period" : 10,

# skips the first k stats for the plot
"plot_offset" : 1,


"ImagesReconstructHook" : {"period" : 10,
                             "n_images_columns" : 10,
                             "images_indexes" : {
                                                "train" : [0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900],
                                                "validation" : [0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900],
                                                },
                            },

"PCALatentVariablesHook" : {"period" : 50,
                             },
			      
"dirName" : "./temp",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {0,1},
            "cores_per_GPU" : 2,
	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

}
]
