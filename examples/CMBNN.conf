# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
    {
        'dataName': 'CMB',
        # OLD ONE
	#"data_dir": '/home/hector/CMB-omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s10+pic_s224',
	"data_dir": '/home/hector/CMB_data_finomega_cdm_A_s-uniform-pppr_rot0-pppr_eq2-pppr_lat1-aug1+patch_s10+pic_s224',
	
        #'/home/hector/Test1/Prueba_other_parameters/Folder_Images_array_set/',
        # "data_dir" : '/data1/Folder_Images_array_set/', #captamerica
        "augm_data": 0,
        "only_center": True,
        "Normalize_data":1,
        "debug_mode": False,
	"parameters" : [['omega_b','omega_cdm','A_s']],
	"fraction_dataset" : [100],
        "id_note": "Second_result"
    },
    {
        "stochastic": 0,  # ignore this for the moment
        "stochastic_noise_param": [0],  # ignore this for the moment
        "dtype": "float32",  # datatype for weights, use float32

        "task": "regression",  # it can be classificiation or regression

	"optimizer": [("AdamOptimizer", {"learning_rate" : 0.001,	
		    			"beta1": 0.9,	 
                    	 		"beta2": 0.999}),
	     	     ("GradientDescentOptimizer", {"learning_rate" : 0.005})],

        "network_creator" : ['res_18'],

        "bit_flip": [0],
    # applies a bit flip with proability 0, ignore for the moment
        "drop_out": [0],  # dropout, ignore for the moment
        "batch_size": [50, 80],  #
        "batch_size_test": [50, 80],  #
        "regularizer": [0],  # not implemented for the moment
	# "weights_init" : [("contrib.layers.xavier_initializer",{})], # list of (initializer, initializer_kwargs). the initializer must be possible to find as tf.initializer
        # "bias_init" : [("constant_initializer", {"value" : 0.1})],
	      "activation": ["elu"],  # you can also try "relu"
        "epochs": 2200,  # number of epochs
    },
    {
        "check_ops": 0,  # for debug purposes

        "n_epoch_log": 1,  # how often I ouput logs on std
        "n_epoch_log": 1,  # how often I ouput plot on file

        "save_model": 1,  # if 1, the model is savedevery X epocs
        "n_epoch_save_model": 10,  # X

        "log_gradient": 0,  # ignore for the moment, needs to be reimplemented
        "n_epoch_log_gradient": 1,  #

        "log_tf": 0,  # ignore this
        "verbose": 1,  # if 1, the algorithm prints comments

        # "launcher": "CMBLauncher",
        #"launcher": "training.core.TrainingLauncher.CMBLauncher",

        # do not change this folder, use a copy of this file instead
        "dirName": "temp",  # folder where data is saved
        # "dirName" : "/data1/CMB_Second_try",  	        # folder where data is saved
        # experiments should besaved in /data2/XXX (foca) or /data/XXX
        # (all the rest)
        "seed": 0,  # do not change
        "runs": 1,  # number of run the algorithm is executed
        # set pool size for GPU

        "nodes": [{"used_GPUs": {0,2},
                   "cores_per_GPU": 1,
                   "IP": "localhost"
                   # num_consumers = multiprocessing.cpu_count() * 2
                   }
                  ]
    }
]
