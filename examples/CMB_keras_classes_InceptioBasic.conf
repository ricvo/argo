[
{
"dataName" : "CMB_Pol",
#"data_dir" : "/ssd_data/CMB/Dataset-256-20/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s20+pic_s256",
#"data_dir" : '/ssd_data/CMB/Dataset-256-20-Lens-tools-01noise/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s20+pic_s256/',
#"data_dir" : '/ssd_data/CMB/hector/FullMap/New2019/Dataset-256-20-Lens-tools-01noise/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s20+pic_s256/', #iroman
#"data_dir" : '/data2/CMB/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s20+pic_s256/', #wonderwoman
#"data_dir" :  '/ssd_data/CMB/hector/three_params_July/omega_b_omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0-cod_lt+patch_s20+pic_s256/',#iroman
#"data_dir" :  '/ssd_data/CMB/omega_b_omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0cod_lt+patch_s20+pic_s256/',#capmerica
'data_dir': '/ssd_data/CMB/Test_for_seven_params/omega_b_omega_cdm_A_s_n_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0-cod_hp+patch_s20+pic_s256/',##test7paramscaptamerica
#"data_dir"  :  '/ssd_data/CMB/omega_b_omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0-cod_lt+patch_s20+pic_s256/', #WM and thor
#"data_dir"  :  '/ssd_data/CMB/POL/PolarH0/H0_omega_b_omega_cdm_A_s_n_s_tau_reio_N_ur-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0-cod_hp+patch_s20+pic_s256/', #thorseven
"fraction_dataset" : [100],   #(0,1); 0 if works with total dataset
"parameters" : [['n_s',  'A_s', 'omega_cdm', 'omega_b']],
},
{

"stochastic" : [0],
"stochastic_noise_param" : [0], #, 0.001,0.01],

"model" : "Prediction.RegressionModel",      # it can be classificiation or regression

"network" : "PredictionKerasNetwork",
#"network_architecture" : [
#             [("bayesian_vgg", {}), # {"filters" : [16, 192, 384, 384, 256]}),
#             [("Alex_net_modif", {}),
#             ("MultivariateNormalTriL", {"bayesian_layers" : True})]],

"network_architecture" : [
              [
#               ("BayesianVgg", {
#                        "filters" : [16, 16, 32, 32, 32],#[32, 64, 64, 128, 128],
#                        "kernels" : [3, 3, 3, 3, 3],
#                        "strides" : [2, 2, 2, 2, 2],
#                        "renorm" : rn,
#                       "flipout": fl,
#                       "pooling" : pool}),
#               ("MultivariateNormalTriL", {"bayesian_layers" : bl, "flipout" : fl})]
#               for bl in [False]
#               for rn in [True]
#               for fl in [True]
#               for pool in ["max"] #, "avg", None]
              ("BayesianInception_Basic_residual", {#BayesianResNet #BayesianInception
                       "filters_1x1":[16, 16, 16, 32],
                       "filters_3x3_reduce":[32, 32, 32, 32],
                       "filters_3x3":[32, 32, 32, 32],
                       "filters_5x5_reduce":[16, 16, 16, 32],
                       "filters_5x5":[16, 16, 16, 32],
                       "filters_pool_proj":[32, 32, 32, 32],
                       "strides":[2, 2, 2, 2],
                       "renorm" : rn,
                       "flipout": fl,
                       "pooling" :'max', #avg 
#                       "activation": ('relu', {}),
                       "activation": ("elu", {})
                      }
               ),
              ("MultivariateNormalTriL", {"bayesian_layers" : bl, "flipout" : fl})]
              for bl in [False]
              for rn in [True]
              for fl in [True]
	      ],

"init_reg_kwargs" : [{

           "kernel_initializer" : ("glorot_normal_initializer", {}),
           "bias_initializer": ("initializers.constant", {"value" : bi}),

           "kernel_regularizer": ("contrib.layers.l2_regularizer", {"scale" : klreg}), # ("keras.regularizers.l2", {"l" : 5e-6}), None,
           "bias_regularizer": ("contrib.layers.l2_regularizer", {"scale" : blreg}), # ("keras.regularizers.l2", {"l" : 5e-6}), #None,

           "activity_regularizer": None,
        }
         for klreg in [5e-2]
         for blreg in [5e-2]
         for bi in [0.1]
        ],

"init_reg_kwargs_bayes" : [
        {
        "posterior" :
             {
             "kernel_untr_scale_initializer" : ("initializers.random_normal", {"mean" : -9., "stddev" : 1e-2}),
             "kernel_loc_initializer" : klinit,

             "bias_loc_initializer" : ("initializers.constant", {"value" : bli}),

             "kernel_loc_regularizer" : ("keras.regularizers.l2", {"l" : 5e-4}), #("contrib.layers.l2_regularizer", {"scale" : 1e-5}),
             "kernel_untr_scale_regularizer" : ("keras.regularizers.l2", {"l" : 5e-4}), #("contrib.layers.l2_regularizer", {"scale" : 1e-5}), #("sum_regularizer", {"scale" : ksreg}),
             "bias_loc_regularizer" : ("keras.regularizers.l2", {"l" : 5e-4}), #("contrib.layers.l2_regularizer", {"scale" : 1e-5}),

             "kernel_untr_scale_constraint_max" : -1.6,

             "activity_regularizer": None,
             },

        "prior" : prior_kwargs,
        }

         for bli in [10.] #, 1., 10.
         for klinit in [
             #("initializers.random_normal", {"mean" : 0., "stddev" : 1e-2}),
             #("initializers.random_normal", {"mean" : 0., "stddev" : 1e-4}),
             ("glorot_normal_initializer", {})
         ]
#          for prior_untr_scale in [-3.] #, 3.]
        for prior_kwargs in [
#                      {
#                      "default" : True
#                      },
                     {
                     "default" : False,
                     "kernel_loc_initializer" : ("initializers.constant", {"value" : 0.01}),
                     "kernel_untr_scale_initializer" : ("initializers.constant", {"value" : 1.}),
                     "trainable" : True,
                     }
        ]
],


#aleatoric / stochastic prediction
#"cost_function": [("Likelihood", {'use_alpha':False, 'alpha_parameter':0,"alpha_samples":1})],
"cost_function": [("Likelihood", {'use_alpha':True, 'alpha_parameter':0.6, "alpha_samples":3})], ## use 1, if use_alpha is False

"optimizer":  [ ("AdamOptimizer", {"learning_rate" :
                                        (1e-9,
                                        "exponential_decay", {"learning_rate" : 1e-5,
                                                              "decay_steps" : damp_steps, #flipout: 4000/6000, reparametrization:8000
                                                              # total train samples: 43200. 43200/32 = 1350 steps per epoch
                                                              "decay_rate" : 0.9,
                                                              "staircase" : True
                                                             }
                                         ),
                                  "beta1" : 0.9,
                       			  "beta2" : 0.999})
                for damp_steps in [4500]
                ] ,

"grad_clipping" : ("clip_by_global_norm", {"value" : 100}),

"batch_size_train" : [16],
"batch_size_eval" : [16],


"epochs" : 400,                     # number of epochs

},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"save_model" : 1,		# if 1, the model is savedevery X epocs
"save_model_period" : 20,      #0.1,

"plot_offset" : 10,

"save_summaries" : 1,
"save_summaries_period" : 5,

#"GradientsHook" : {"period" : 10},

"CorrelationHook" : {"period" : 10},

"ImagesInputHook" : {"period" : 10,
                    "how_many" : 6,
                    "n_images_columns" : 6,
                    "until": 100},

"MCDropoutHook" : {"period" : 400,
                    "n_batches" : 80,
                    "posterior_samples" : 3000},

"WeightsHistogramHook" : {"period" : 10},

"dirName" : "/data1/mcmc_keras/bayesian_Incep_Basic_test_threeparams2drop/",
	    			# experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,			# do not change
"runs" : 1,			# number of run the algorithm is executed
# set pool size for GPU


"nodes" : [{"used_GPUs" : [2],

            "cores_per_GPU" : 1,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
