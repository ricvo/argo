[
{
"dataName" : "CMB",
#"data_dir" : "/ssd_data/datasets/CMB/FullMap/withoutnoisy/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq2-pppr_lat1-aug0+patch_s10+pic_s256",
"data_dir" :'/ssd_data/CMB/omega_cdm_A_s-uniform-pppr_rot0-pppr_eq1-pppr_lat1-aug0+patch_s20+pic_s256/', 
"vect" : False,       # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
"parameters" : ['omega_cdm', 'A_s']

},
{
"stochastic" : 0,
"stochastic_noise_param" : [0],
#"dtype" : "float32",            #datatype for weights, use float32
"model" : "Prediction.RegressionModel",      # it can be classificiation or regression

"cost_function": [("Likelihood", {})],

"optimizer":  [  	("AdamOptimizer", {"learning_rate" : lr,
		       			  "beta1" : 0.9,
                       			  "beta2" : 0.999}) for lr in [0.0001] ] ,
	     #("AdamOptimizer", {"learning_rate" : lr,
	      #				  "beta1": 0.9,
              #        			  "beta2":0.999}) for lr in [0.001, 0.0002, 0.0003, 0.0005, 0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 1]
	     #],
		       #("AdamOptimizer", {"learning_rate" : 0.0001,
		       #			  "beta1": 0.9,
                       #			  "beta2":0.999}),
#		       ("PeriodicInvCovUpdateKfacOpt", {"learning_rate" : 0.0001,
#                				        "cov_ema_decay" : 0.95,
#                				        "damping" : 0.001,
#                				        "momentum" : 0.9,
#							"invert_every" : 10,
#							"cov_update_every" : 1})
#		      ],


"grad_clipping" : ("clip_by_global_norm", {"value" : 5}), 
#"grad_clipping" : ("clip_by_norm", {"value" : 1}), 
#"grad_clipping" : (None, {}), 


#"grad_clipping" : {"method" : "global", "value" : 3.},

"batch_size_train" : [32],
"batch_size_test" : [32],

#"rescale" : [1e-4], # rescale for continuous datasets

# in the specification of each layer you can use any function in tf.layers.
# Any argument can be passed in the form of kwargs (also positional arguments have
# to be passed with their respective key)
"network_architecture" : [  # (sntModule, kwargs, bool_activation)  bool_activation determines wheter to apply activations or not after the layer
		           [
		             #("ConvNet2D", {"output_channels": [64, 64], "kernel_shapes": [3], "strides": [1], "paddings" : ["SAME"]}, 1),
             		     #("MaxPooling2D", {"pool_size": (2, 2), "strides": (2, 2)}, 1),
			     #("ConvNet2D", {"output_channels": [128, 128], "kernel_shapes": [3], "strides": [1], "paddings" : ["SAME"]}, 1),
             		     #("MaxPooling2D", {"pool_size": (2, 2), "strides": (2, 2)}, 1),
			     #("ConvNet2D", {"output_channels": [256, 256], "kernel_shapes": [3], "strides": [1], "paddings" : ["SAME"]}, 1),
             		     #("MaxPooling2D", {"pool_size": (2, 2), "strides": (1, 1)}, 1),
			     #("ConvNet2D", {"output_channels": [512, 512, 512, 512], "kernel_shapes": [3], "strides": [1], "paddings" : ["SAME"]}, 1),			     
             		     #("MaxPooling2D", {"pool_size": (2, 2), "strides": (2, 2)}, 1),
			     #("ConvNet2D", {"output_channels": [512, 512], "kernel_shapes": [3], "strides": [1], "paddings" : ["SAME"]}, 1),
             		     #("MaxPooling2D", {"pool_size": (2, 2), "strides": (2, 2)}, 1),
             		     #("Linear", {"output_size": 2000}, 1),
			     ("ConvNet2D", {"output_channels": [16, 32, 64, 128, 256], "kernel_shapes": [[7,7], [5,5], [5,5], [3,3], [3,3]], "strides": [2,2,2,2,2], "paddings" : ["SAME"]}, 1),
			     ("BatchNorm", {"offset" : 1, "scale" : 1, "decay_rate" : 0.9}, 1),
			     
			     ("BatchFlatten", {}, 0),
             		     ("Linear", {"output_size": 1024}, 0),
			     ("Dropout", {"rate" : 0.5}, 1),
             		     ("Linear", {"output_size": 256}, 1),
			     ("GaussianDiagonal", {"module_tuple" : ("LinearWN", {"output_size" : 2,
                                                                     "use_weight_norm" : False}),
				                   "minimal_covariance": 0.0,
						   "covariance_parameterization" : "softplus",
                                  }, 0)
			]
        ],


"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : scale }) for scale in [0.01, 0.05]],
"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : scale }) for scale in [0.01, 0.05]],

"activation" : [ "relu", "elu"], # "elu"  you can also try "relu"
"weights_init" : [("contrib.layers.xavier_initializer",{})],
"bias_init" : [("constant_initializer", {'value' : 0.1})],

"epochs" : 500,                     # number of epochs
},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"n_epoch_log" : 1,              # how often I ouput logs on std

"save_model" : 1,		# if 1, the model is savedevery X epocs
"save_model_period" : 100,      #0.1,

"plot_offset" : 1,

"save_summaries" : 0,
"save_summaries_period" : 10,

"GradientsHook" : {"period" : 5},

#"HessianHook" : {"period" : 1, "create_heatmap" : 0},


#"ImagesInputHook" : {"epoch_period" : 50,
#                    "how_many" : 18,
#                    "n_images_columns" : 6},


"dirName" : "temp",
	    			# experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,			# do not change
"runs" : 1,			# number of run the algorithm is executed
# set pool size for GPU

"nodes" : [{"used_GPUs" : {0,1,2,3},
            "cores_per_GPU" : 1,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
