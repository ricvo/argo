Description of the options used for training VAE

##############################################################
[
{
"dataName" : # "mnist-c" (continuous MNIST) / "mnist-d" (binarized MNIST) /"cifar10"
"stochastic" : # 0/1 -- sample from the dataset after every epoch during training if flag is set 
"stochastic_noise_param" : # 0.0001 (e.g.) -- variance of the zero-mean Gaussian noise added to continuos data
"obs" : # type=list, models for the continuous distribution p(x|z) 
        # 0 = diagonal Gaussian
        # 3 = grid graphical model --> Gaussian with block tridiagonal precision 

"warm_up_method" : #0/1 gradually introduce the KL in the cost function over 50 epochs

#different types of perturbations added to the covariance of the posterior to avoid numerical issues
#not needed anymore, to be removed (set to 0 meanwhile)
"epsilon" : 0,
"epsilon_obs" : 0,

#different types of perturbations added to the latent samples or dataset (not being used anymore, to be #removed, set to 0 meanwhile)
"bit_flip" : [0],
"drop_out" : [0],
"latent_noise" : [0],

# different types of cost functions: 0 = VI bound, 1 = Renyi
"cost_function": [{"cost" : 0}],

# learning reate for gradient descent
"learning_rate" : [0.0003],

# number of samples from the posterior to approximate the expectation
"samples" : [1],

# number of importance samples to approximate the log marginal distribution
"imp_samples" : [200], 

"batch_size" : [100],
"hidden_variables_size" : [200],
"latent_variables_size" : [20], 

# coefficient of L2 regularization
"regularizer" : [0],

# types of initializations of the weights
"weights_init" : ["xavier_init"],

# different types of models for the posterior (type = list)
"k" : # 0 - diagonal Gaussian,
      # 1 - rank 1 update of diagonal,
      # -2 - tridiagonal precision,
      # 3 - block tridiagonal precision,

# type of transfer function (type = list)      
"transfer_fct" : ["relu"],


"epochs" : 1000,
# not used anymore, to run on cpu, set used_GPUs = {-1} and then run single
#cpu : 1
},
{
# enable to check for NaNs or Infs in the graph
"check_ops" : 0,
#"log_txt" : 0,

# save the model if flag is set every 100 epochs
"save_model" : 1,
"n_epoch_save_model" : 100,

# log statistics of the gradient every 5 epochs
"log_gradient" : 0,
"n_epoch_log_gradient" : 5,

# generate 100 images from VAE every 5 epochs
"generate_images": 0/1,
"n_generate_images": 100,
"n_epoch_generate_images" : 5,

# regenerate 100 images from VAE every 5 epochs
"regenerate_images": 0/1,
"n_regenerate_images": 100,
"n_epoch_regenerate_images" : 5,


# log statistics of the latent parameters for image number 10 in the dataset every 5 epochs
"log_latent_vars_model" : 0/1,
"n_epoch_log_latent_vars_model" : 5,
"log_latent_vars_model_points" : [10], # type=list

# log the PCA components of the latent means and samples every 5 epochs
"log_latent_vars_pca" : 0/1,
"n_epoch_log_latent_vars_pca_eigenvals" : 5,

# use tensorboard for visualization
"log_tf" : 0,
"verbose" : 1,

# folder where to save logs
"dirName" : "./myFolder",

# set seed and number of runs of each experiment
"seed" : 0,
"runs" : 10,

# set pool size for GPU
"used_GPUs" : {0,1,2,3},
"cores_per_GPU" : 2   #  num_consumers = multiprocessing.cpu_count() * 2
}
]
