[
{
"dataName" : "FashionMNIST",  #database name
"binary" : 0,          #we have two version of MNIST, continuous and binarized, use the continuous
"flip_fraction" : 0,
"vect" : False         # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{
"stochastic" : 1,
"stochastic_noise_param" : [0.01],
"rescale" : [1e-5], # rescale for continuous datasets

"model" : "Prediction.ClassificationModel",      # it can be classificiation or regression
"preprocess" : [("FromAE", {"filename": "/data/thor_hd1/FashionMNIST-st0/VAE-cELBO_b1.0_wuW5-d0-st1-stp0.01-re1e-05-bs32-trA_lr0.0001_bo0.9_bt0.999-cV10.0-e_CN-o256-512-1024k3-3s2-2-1_GDd128-d_D50176_BR_CNT-o512-256-1k3-3s1-2-2_LNDd.mc0.01-cpS-aR-wix-bic0.1-wrLtwo0.0001-brLtwo0.0001-r0/saved_models/frozen_transform-860718.pb",
                           "transform_prob": tp,
                           "noisy_transform_prob": np})
                           for tp in [0.3] for np in [0.1, 0.3, 0.7]],

"cost_function": [("CrossEntropyWithLogits", {})],

"optimizer":  [("AdamOptimizer",
                 {"learning_rate" : 0.001,
                 "beta1": 0.9,
                 "beta2":0.999}),
                         ],


"grad_clipping" : ("clip_by_global_norm", {"value": 100}),

"batch_size_train" : [32],        #
"batch_size_test" : [32],  #


# in the specification of each layer you can use any function in tf.layers.
# Any argument can be passed in the form of kwargs (also positional arguments have
# to be passed with their respective key)
"network_architecture" : [  # (sntModule, kwargs, bool_activation)  bool_activation determines wheter to apply activations or not after the layer
        [
                ("Conv2D", {"output_channels" : 32, "kernel_shape" : (3,3)}, 1),
                ("Conv2D", {"output_channels" : 32, "kernel_shape" : (3,3)}, 1),
                ("BatchFlatten", {}, 0),
                #("Conv2D", {"output_channels" : 5, "kernel_shape" : (3,3)}, 1),
                ("Linear", {"output_size" : 200}, 1),
                ("Linear", {"output_size" : 10}, 0)
                ]
        ],

"activation" : ["relu"],
"weights_init" : [("contrib.layers.xavier_initializer",{})],
"bias_init" : [("constant_initializer", {'value' : 0.1})],

"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : reg}) for reg in [0.0001, 0.001, 0.01]],
"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : reg}) for reg in [0.0001]],

"epochs" : 300,                     # number of epochs
},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"n_epoch_log" : 1,              # how often I ouput logs on std

"save_model" : 1,               # if 1, the model is savedevery X epocs
"save_model_period" : 20, #0.1,

"plot_offset" : 1,

"save_summaries" : 1,
"save_summaries_period" : 10,

"GradientsHook" : {"period" : 10},

#"HessianHook" : {"period" : 1, "create_heatmap" : 0},
"ImagesInputHook" : {"period" : 20,
                    "how_many" : 30,
                    "n_images_columns" : 10},


"dirName" : "/data1",
                                # experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,                     # do not change
"runs" : 1,                     # number of run the algorithm is executed
# set pool size for GPU

"nodes" : [{"used_GPUs" : [3,2,1,0],
            "cores_per_GPU" : 1,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
