[
{
"dataName" : "GTSRB",  #database name
"binary" : 0,          #we have two version of MNIST, continuous and binarized, use the continuous
"flip_fraction" : 0,
"vect" : False         # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)

},
{
"stochastic" : 2,
"stochastic_noise_param" : [0.1],
#"dtype" : "float32",            #datatype for weights, use float32
"model" : "Prediction.ClassificationModel",      # it can be classificiation or regression

"cost_function": [("CrossEntropyWithLogits", {})],

"optimizer":  [("AdamOptimizer",
		        {"learning_rate" : 0.0001,
                 "beta1": 0.9,
                 "beta2":0.999}),
			 ],

"grad_clipping" : ("clip_by_global_norm", {"value": 100.0}),

"batch_size_train" : [100],        #
"batch_size_test" : [100],  #


# in the specification of each layer you can use any function in tf.layers.
# Any argument can be passed in the form of kwargs (also positional arguments have
# to be passed with their respective key)
"network_architecture" : [  # (sntModule, kwargs, bool_activation)  bool_activation determines wheter to apply activations or not after the layer
        [
        ("VGGBlock", {"channels" : 64, "kernel_shape" : 3, "prob_drop": 0.1, "features_name": "perceptual_features1"}, 0),

        ("VGGBlock", {"channels" : 64, "kernel_shape" : 3, "prob_drop": 0.1, "features_name": "perceptual_features2"}, 0),

        # ("VGGBlock", {"channels" : 32, "kernel_shape" : 3, "prob_drop": 0.1}, 0),
        # ("Identity", {"name" : "perceptual_loss_features1"}, 0),

        # ("VGGBlock", {"channels" : 64, "kernel_shape" : 3, "prob_drop": 0.1}, 0),
        # ("Identity", {"name" : "perceptual_loss_features2"}, 0),

		("BatchFlatten", {}, 0),
		("Linear", {"output_size" : 512}, 1),
		("Linear", {"output_size" : 43}, 0)
		]
        ],

"activation" : ["relu"],

"weights_init" : [("contrib.layers.xavier_initializer",{})],
"bias_init" : [("constant_initializer", {'value' : 0.1})],

"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : reg}) for reg in [0.001]],
"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : reg}) for reg in [0.001]],


"epochs" : 3,                     # number of epochs
},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"n_epoch_log" : 1,              # how often I ouput logs on std

"save_model" : 1,		# if 1, the model is savedevery X epocs
"save_model_period" : 1, #0.1,
"save_pb" : 1,

"plot_offset" : 1,

"save_summaries" : 0,
"save_max_to_keep" : 3,
"save_summaries_period" : 10,

"GradientsHook" : {"period" : 10},

#"HessianHook" : {"period" : 1, "create_heatmap" : 0},


#"ImagesInputHook" : {"epoch_period" : 50,
#                    "how_many" : 18,
#                    "n_images_columns" : 6},


"dirName" : "/data3/test_vgg_perc_features/renorm",
	    			# experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,			# do not change
"runs" : 1,			# number of run the algorithm is executed
# set pool size for GPU

"nodes" : [{"used_GPUs" : {4},
            "cores_per_GPU" : 1,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
