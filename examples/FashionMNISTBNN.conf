##############################################################
[
{
"dataName" : "FashionMNIST",  #database name
"vect" : False         # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"stochastic" : [1, 2],
"stochastic_noise_param" : [0.1],


"model" : "Prediction.ClassificationModel",      # it can be classificiation or regression

"n_samples_train" : 7,
"cost_function": ("CrossEntropy", {}),

"network" : "PredictionKerasNetwork",

"network_architecture" : [
                      [("BayesianVgg", {
                               "filters" : [32, 128],#[32, 32, 32, 32, 64],#[32, 64, 64, 128, 128],#[16, 16, 32, 32, 32]
                               "kernels" : [3, 3],
                               "strides" : [2, 2],
                               "renorm" : True,
                              "flipout": True,
                              "linear_last" : [512, 128],
                              "final_activation" : True,
                              "pooling" : pool}),
                       ("OneHotCategorical", {"output_size" : 10}), # no distribution     #   ("MultivariateNormalTriL", {"bayesian_layers" : bl, "flipout" : fl}),
                       None  # no flow     #  flow_params
                      ]
                      for pool in ["max"] #, "avg", None]
                  ],

"init_reg_kwargs" : [{

           "kernel_initializer" :("glorot_normal_initializer", {}),
           "bias_initializer": ("initializers.constant", {"value" : 0.3}),

           "kernel_regularizer": ("contrib.layers.l2_regularizer", {"scale" : klreg}),
           "bias_regularizer": ("contrib.layers.l2_regularizer", {"scale" : blreg}),

           "activity_regularizer": None,
        }
        for klreg,blreg in [(1e-3, 1e-3)]
        ],

"init_reg_kwargs_bayes" : [
        {
        "posterior" :
             {
             "kernel_untr_scale_initializer" : ("initializers.random_normal", {"mean" : -9., "stddev" : 1e-2}),
             "kernel_loc_initializer" : klinit,

             "bias_loc_initializer" : ("initializers.constant", {"value" : bli}),

             "kernel_loc_regularizer" : ("contrib.layers.l2_regularizer", {"scale" : klreg}),
             "kernel_untr_scale_regularizer" : ("contrib.layers.l2_regularizer", {"scale" : kusreg}),
             "bias_loc_regularizer" : ("contrib.layers.l2_regularizer", {"scale" : blreg}),

             "kernel_untr_scale_constraint_max" : -1.6,

             "activity_regularizer": None,
             },

        "prior" : prior_kwargs,
        }
         for bli in [0.3] #, 1., #flip10.
         for blreg, klreg, kusreg in [(1e-3,1e-3,1e-7), (1e-4,1e-4,1e-7)] #(1e-9,1e-9,1e-9),
         for klinit in [
             ("glorot_normal_initializer", {})
         ]


        for prior_kwargs in [
                      {
                      "default" : True
                      },
#                      {
#                      "default" : False,
#                      "kernel_loc_initializer" : ("initializers.constant", {"value" : 0.}),
#                      "kernel_untr_scale_initializer" : ("initializers.constant", {"value" : 1.}),
#                      "trainable" : True,
#                      }
        ]

],


"optimizer":  [ ("AdamOptimizer", {"learning_rate" :
                                        (mlr,
                                        "exponential_decay", {"learning_rate" : ilr,
                                                              "decay_steps" : ds, # total train samples: TOT. TOT/BATCH_SIZE = SE steps per epoch
                                                              "decay_rate" : 0.9,
                                                              "staircase" : True
                                                             }
                                         ),
                                  "beta1" : 0.9,
                       			  "beta2" : 0.999})
                   for mlr, ilr, ds in [(1e-7, 1e-4, 2000)] #[(1e-3, 2000), (1e-4, 3000)]
                   ] ,

"grad_clipping" : ("clip_by_global_norm", {"value" : 100}),

"batch_size_train" : [32, 128],
"batch_size_eval" : [32],


"epochs" : 80,                     # number of epochs

},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"save_model" : 1,		# if 1, the model is savedevery X epocs
"save_model_period" : 20,      #0.1,

"plot_offset" : 3,

"save_summaries" : 0,
"save_summaries_period" : 5,

#"GradientsHook" : {"period" : 10},

# "CorrelationHook" : {"period" : 30,
#                      "n_samples" : 10},

"ImagesInputHook" : {"period" : 1,
                     "how_many" : 6,
                     "n_images_columns" : 6,
                     "until": 2},

# "MCDropoutHook" : {"period" : 180,
#                     "n_batches" : 80,
#                     "posterior_samples" : 2500},

"MCClassificationHook" : { "period" : 80,
                            "n_batches" : 80,
                            "posterior_samples" : 500},

"WeightsHistogramHook" : {"period" : 30},

"dirName" : "/data2/mcmc_classification/BVGGfix",

	    			# experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,			# do not change
"runs" : 1,			# number of run the algorithm is executed
# set pool size for GPU


"nodes" : [{"used_GPUs" : [2],
            "cores_per_GPU" : 1,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
