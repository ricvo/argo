# see https://www.kaggle.com/richbrosius/iris-classification-using-tensorflow
# see https://www.kaggle.com/wellionx/tensorflow-neural-network-tutorial-with-iris-data
[
{
"dataName" : "IRIS",  #database name
"binary" : 0,          #we have two version of MNIST, continuous and binarized, use the continuous
"flip_fraction" : 0,
"vect" : False         # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)

},
{
"stochastic" : 0,
"stochastic_noise_param" : [0],
#"dtype" : "float32",            #datatype for weights, use float32
"model" : "Prediction.ClassificationModel",      # it can be classificiation or regression


"cost_function": [("UnfusedCrossEntropyWithLogits", {})],

# ("NewtonMethodOptimizer", {"learning_rate" : 0.001, "damping" : 0.1})

"natural_gradient": [0],
"optimizer": [ ("GradientDescentOptimizer", {"learning_rate" : lr})
	     	       for lr in [0.05,0.01,0.005,0.001,0.0005,0.0001]],
	               # ("AdamOptimizer", {"learning_rate" : lr,
		       #			  "beta1": 0.9,
                       #			  "beta2":0.999}) for lr in [0.005, 0.0002, 0.0003, 0.0005, 0.001, 0.002, 0.003, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 1]
		       #("AdamOptimizer", {"learning_rate" : 0.0001,
		       #			  "beta1": 0.9,
                       #			  "beta2":0.999}),
#		       ("PeriodicInvCovUpdateKfacOpt", {"learning_rate" : 0.0001,
#                				        "cov_ema_decay" : 0.95,
#                				        "damping" : 0.001,
#                				        "momentum" : 0.9,
#							"invert_every" : 10,
#							"cov_update_every" : 1})
#		      ],


"grad_clipping" : ("clip_by_global_norm", {"value" : 1}), # NB not implemented in the filename
#"grad_clipping" : ("clip_by_norm", {"value" : 1}), # NB not implemented in the filename
#"grad_clipping" : (None, {}), # NB not implemented in the filename


#"grad_clipping" : {"method" : "global", "value" : 3.},

"batch_size_train" : [3, 10, 32, 64, 120],        #
"batch_size_test" : [120],  #

#"rescale" : [-1], # rescale for continuous datasets

# in the specification of each layer you can use any function in tf.layers.
# Any argument can be passed in the form of kwargs (also positional arguments have
# to be passed with their respective key)
"network_architecture" : [  # (sntModule, kwargs, bool_activation)  bool_activation determines wheter to apply activations or not after the layer
        	[
		        # try also 10 20 10
			("Linear", {"output_size" : 10}, 1),
			("Linear", {"output_size" : 3}, 0)
		]
#		[
#			("Conv2D", {"output_channels" : 10, "kernel_shape" : (3,3)}, 1),
#               	("BatchFlatten", {}, 0),
#			("Linear", {"output_size" : 200}, 1),
#			("Linear", {"output_size" : 10}, 0)],
#		[
#			("Conv2DWN", {"output_channels" : 10, "kernel_shape" : (3,3),  "use_weight_norm" : False}, 1),
#	        	("BatchFlatten", {}, 0),
#			("Linear", {"output_size" : 200}, 1),
#			("Linear", {"output_size" : 10}, 0)
#		]
        ],

"activation" : ["relu"], #, "elu", "sigmoid" # you can also try "relu"
"weights_init" : [("contrib.layers.xavier_initializer",{})],
"bias_init" : [("constant_initializer", {'value' : 0.1})],

"epochs" : 2000,                     # number of epochs
},
{
"check_ops" : 0,                # for debug purposes

"time_reference" : "epochs",
"stats_period" : 1,

"n_epoch_log" : 1,              # how often I ouput logs on std

"save_model" : 1,		# if 1, the model is savedevery X epocs
"save_model_period" : 100, #0.1,

"start_plot_epoch" : 3,

"save_summaries" : 0,
"n_epoch_save_summaries" : 10,

"GradientsHook" : {"period" : 5},


"dirName" : "natural",
	    			# experiments should besaved in /data2/XXX or /data1/XXX

"seed" : 0,			# do not change
"runs" : 1,			# number of run the algorithm is executed
# set pool size for GPU

"nodes" : [{"used_GPUs" : {3},
           "cores_per_GPU" : 3,
            "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
            }
            ]
}
]
