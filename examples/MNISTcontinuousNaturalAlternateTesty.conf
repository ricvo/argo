# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "MNIST",
"binary" : 1,
"stochastic" : 1,
"flip_fraction" : 0.,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"model" : ["HM.HM"],
"grad_clipping" : (None, {}),
#"grad_clipping" : ("clip_by_norm", {"value" : 500}),
#"grad_clipping" : [x for x in [("clip_by_global_norm", {"value" : 100}),("clip_by_global_norm", {"value" : 10}), ("clip_by_value", {"value" : 10}), ("clip_by_norm", {"value" : 10}) ]],
"clip_probs": 1e-3,
"note" : "compK",
"stochastic" : [0],
"stochastic_noise_param" : [0.001], #0, ,0.00001,0.001,0.1,1

"denoising" : [0],

"optimizer": [(opt, {"learning_rate" : lr,
					 "individual_learning_rate": ilr,
					 "diagonal_pad":dp,
					 "rescale_learning_rate": rr,
					 "k_step_update": ks, "q_baseline": True
						})
                        for opt in ["WakeSleepOptimizer",
									"ReweightedWakeSleepOptimizer",
									"WakeSleepOptimizerAdam",
									"ReweightedWakeSleepOptimizerAdam",
									"NaturalWakeSleepOptimizerAlternate",
									"NaturalWakeSleepOptimizerAlternateAdam",
									"NaturalWakeSleepOptimizer",
									"NaturalWakeSleepOptimizerAdam"][4:5]
	    for lr in [0.0001,
				   0.001,
				   0.05,
				   0.1,
				   1.0,
				   0.01,
				   (0.000001, "polynomial_decay", {"learning_rate": 0.01,"decay_steps":10000,"end_learning_rate":0.0, "power":1.0}),
                   (0.000001, "exponential_decay", {"learning_rate": 0.01,"decay_steps":250,"decay_rate":0.95})
				  ][1:2]
		for rr in [1.0]
		for dp in [(0.00001, "polynomial_decay", {"learning_rate": 0.001,"decay_steps":50000,"end_learning_rate":11.0, "power":1.0}),
				   (0.00001, "polynomial_decay", {"learning_rate": 0.001,"decay_steps":10000,"end_learning_rate":11.0, "power":1.0}),
				   (0.00001, "polynomial_decay", {"learning_rate": 0.0001,"decay_steps":50000,"end_learning_rate":11.0, "power":1.0}),
				   (0.00001, "polynomial_decay", {"learning_rate": 0.0001,"decay_steps":10000,"end_learning_rate":11.0, "power":1.0}),
				   (0.00001, "exponential_decay", {"learning_rate": 0.0001,"decay_steps":1000,"decay_rate":1.05}),
				   (0.00001, "exponential_decay", {"learning_rate": 0.0001,"decay_steps":500,"decay_rate":1.05}),
				   (0.00001, "exponential_decay", {"learning_rate": 0.001,"decay_steps":500,"decay_rate":1.05}),
				   (0.00001, "exponential_decay", {"learning_rate": 0.001,"decay_steps":1000,"decay_rate":1.05}),
				    0.001,
					0.002,
					0.005,
					0.01,
					0.02,
					0.05,
					0.1,
					0.2,
					0.5,
					1.0,
					10.1][10:11]
		#for ilr in [[0.15,0.01,0.1]]#, [0.15,0.01,0.1]]
		for ilr in [1.0]#, [0.15,0.01,0.1]]
		for ks in [0,5,10,50,100,500,1000][0:1]
 ],

"cost_function": [("HMLogJointLikelihoodRWS", {})
		  ],

"batch_size_train" : [8,32][1:2],
"batch_size_eval" : 100,

"samples" : [1,5,10][1:2], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [{
            "layers": ll,
        } for ll in [[300,200,100,75,50,35,30,25,20,15,10,10], [400,300,200,100,10], [200,100,10]][2:3]
	],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],

#"weights_init" : [("constant_initializer", {'value' : 0.1})],  # !!! truncated normal
"weights_init" : [("glorot_normal_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],


"activation" : ["elu"],
"epochs" : 5,

},
{
"check_ops" : 0,

# choose steps or epochs
#"time_reference" : "epochs",
"time_reference" : "steps",

# choose to save the model every n steps
"save_model" : 0,
"save_model_period" : 1000, #0.1,

# how often to log stats (txt) and save summaries
"save_summaries" : 0,
"save_summaries_period" : 10,
"stats_period" : 100,

# skips the first k stats for the plot
"plot_offset" : 1,



"GradientsHook" : {"period" : 100},

"ImagesInputHook" : {"period" : 100,
                     "how_many" : 20,
                     "n_images_columns" : 20,
					 "until": 1},

"ImagesReconstructHook" : {"period" : 1000,
                           "n_images_columns" : 16,
                           "images_indexes" : {
                                                 "train" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 "validation" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 },
                            },

"ImagesGenerateHook" : {"period" : 1000,
                        "n_images_columns" : 10,
                        "n_gen_samples" : 100
                       },

#"TwoDimPCALatentVariablesHook" : {"period" : 10,
#                                  },
#
#"PCALatentVariablesHook" : {"period" : 10,
#                             },
#
#
"LogpImportanceSamplingHook" : {"period" : 5000,
                                "n_samples" : [1, 10,100, 1000],
                                "batch_size" : 10,
                                "repetitions" : 1
                       	       },

"HMFisherMatrixHook": {"period" : 100},

"dirName" : "/data1/csongor/MNIST/testyHMFisherHook",
#"dirName" : "temp",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {4},
            "cores_per_GPU" : 2,

	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

}
]
