# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "MNIST",
"binary" : 1,
"stochastic" : 1,
"flip_fraction" : 0.,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"model" : ["HM.HM"],
#"grad_clipping" : ("clip_by_norm", {"value" : 500}),
"grad_clipping" : (None, {}),
"clip_probs": 1e-6,
#"grad_clipping" : [x for x in [("clip_by_global_norm", {"value" : 100}),("clip_by_global_norm", {"value" : 10}), ("clip_by_value", {"value" : 10}), ("clip_by_norm", {"value" : 10}) ]],
"note" : "LR",
"stochastic" : [0],
"stochastic_noise_param" : [0.001], #0, ,0.00001,0.001,0.1,1

"denoising" : [0],

"optimizer": [(opt, {"learning_rate" : lr,  "individual_learning_rate": ilr, "rescale_learning_rate": 1.0, "diagonal_pad":dp, "q_baseline":False})
                        for opt in ["WakeSleepOptimizer", "NaturalWakeSleepOptimizer"][1:2]
						for lr in [0.01][:]
                        for ilr in [1]
                        for dp in [0.01,10.0,1000.0][0:1]
                        #for ilr in [[1,0.1,0.1] ,[0.1,1,0.1], [0.1,0.1,1], [1,1,0.1]]
             ],

"cost_function": [("HMLogJointLikelihoodRWS", {})
#"cost_function": [("HMLogLikelihood", {})
		  ],

"batch_size_train" : 32,
"batch_size_eval" : [i for i in [100]],

"samples" : [1,3,5][2:3], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [{
            "layers": ll,
        } for ll in [[300,200,100,75,50,35,30,25,20,15,10,10],[200,100,10]][1:2]
	],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],
#"weights_init": [i for i in [("glorot_normal_initializer",{})]],
"weights_init": [i for i in [("truncated_normal_initializer",{"mean":0.0,"stddev": 0.1})]],

#"weights_init" : [("constant_initializer", {'value' : 0.1})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],

"activation" : ["elu"],
"epochs" : 20,

},
{
"check_ops" : 0,

# choose steps or epochs
#"time_reference" : "epochs",
"time_reference" : "steps",

# choose to save the model every n steps
"save_model" : 1,
"save_model_period" : 10000, #0.1,

# how often to log stats (txt) and save summaries
"save_summaries" : 0,
"save_summaries_period" : 10000,
"stats_period" : 100,

# skips the first k stats for the plot
"plot_offset" : 1,



"GradientsHook" : {"period" : 10000},

"ImagesInputHook" : {"period" : 1,
                     "how_many" : 20,
                     "n_images_columns" : 20,
					 "until": 1},

"ImagesReconstructHook" : {"period" : 25000,
                           "n_images_columns" : 16,
                           "images_indexes" : {
                                                 "train" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 "validation" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 },
                            },

"ImagesGenerateHook" : {"period" : 25000,
                        "n_images_columns" : 10,
                        "n_gen_samples" : 100
                       },

#"TwoDimPCALatentVariablesHook" : {"period" : 10,
#                                  },
#
#"PCALatentVariablesHook" : {"period" : 10,
#                             },
#
#
"LogpImportanceSamplingHook" : {"period" : 25000,
                                "n_samples" : [1, 10,100, 1000,10000],
                                "batch_size" : 10,
                                "repetitions" : 1
                       	       },

"dirName" : "/data1/csongor/MNIST/quickCompare2",
#"dirName" : "temp",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {4,5,6,7},
            "cores_per_GPU" : 2,

	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

}
]
