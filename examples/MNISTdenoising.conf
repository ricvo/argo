# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "MNIST",
"binary" : 0,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"stochastic" : [1],
"stochastic_noise_param" : [0.01], #0, ,0.00001,0.001,0.1,1
"denoising" : [1],
"rescale" : [0.001],

"minimal_covariance_z" : [0.0],
"minimal_covariance_x" : [0.001],

"perturbed" : 0,
"perturbed_id" : "MNIST-denoising_noisy",

#"perturbed_train_set_x" : "/data/mnist/adversarial/fgsm_exp_eps01_train.npy",
#"perturbed_test_set_x" : "/data/mnist/adversarial/fgsm_exp_eps01_test.npy"


"warm_up_method" : [("warm_up", {"epoch" : 10})],

"optimizer": [{"algorithm" : "AdamOptimizer",
		        "learning_rate" : 0.0001, 'beta1': 0.9,
             'beta2':0.999}
		      ],

"grad_clipping" : ("clip_by_global_norm", {"value" : 3}), # NB not implemented in the filename

#"cost_function": [{"cost" : 2, "alpha" : 0.3}],
"cost_function": [{"cost" : "elbo", "beta" : 1.0},

		  ],

"samples" : [10], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [{
        "encoder" : [
            ("BatchFlatten", {}, 0),
            ("Linear", {"output_size" : 200}, 1),
            ("Linear", {"output_size" : 200}, 1),
    	    ("GaussianDiagonal", {"size" : 20}, 0)],
                                #  "contractive_regularizer" : ("geometric_contractive_regularizer",
                                #                                {"scale_mean" : 0.1,
                                #                                "scale_covariance" : 0.1,
                                #                                "norm" : 2})
				                                 # }, 0)],

        "decoder" : [
            ("Linear", {"output_size" : 200}, 1),
            ("Linear", {"output_size" : 200}, 1),
    	    ("LogitNormalDiagonal", {}, 0)
            ]
	}],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],


"weights_init" : [("contrib.layers.xavier_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],


"activation" : ["relu"],
"epochs" : 1000,

},


{
"check_ops" : 0,

# choose steps or epochs
"time_reference" : "epochs",

# how often to log stats (txt) and save summaries (related to the stats)
"stats_period" : 1,

# choose to save the model every n steps
"save_model" : 1,
"save_model_period" : 10,

# how often to save other specific summaries
"save_summaries" : 1,
"save_summaries_period" : 10,

# skips the first k stats for the plot
"plot_offset" : 1,


"ImagesReconstructHook" : {"period" : 100,
                            "n_images_columns" : 6,
                            "images_indexes" : {
                                                "train" : [0,100,200,300,400,500],
                                                "validation" : [0,100,200,300,400,500],
                                                },
                            },

"ImagesGenerateHook" : {"period" : 100,
                        "n_images_columns" : 6,
                        "n_gen_samples" : 18
                        },

"LogpImportanceSamplingHook" : {"period" : 100,
		       	        "n_samples" : [100],
		     		"batch_size" : 1000,
				"repetitions" : 1
                       	       },


"dirName" : "/data1/denoising_vae",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {3},
            "cores_per_GPU" : 1,
	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],


}
]
