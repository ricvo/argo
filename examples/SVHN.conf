# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "SVHN",
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"stochastic" : [1],
"stochastic_noise_param" : [0.0], #0, ,0.00001,0.001,0.1,1
"denoising" : [0],
"rescale" : [0.001],

"model" : "VAE.VAE", #"VAE_IAF"

"optimizer": [("AdamOptimizer", {"learning_rate" : 0.0001,
					  "beta1": 0.9,
                      			  "beta2":0.999})],

"grad_clipping" : ("clip_by_global_norm", {"value" : 3}), # NB not implemented in the filename

"cost_function": [#("IELBO", {"beta" : 1.0, "h" : 0.01, "normalize" : 1}),
                  ("ELBO", {"beta" : 1.0,
		            "warm_up_method" : ("warm_up", {"epoch" : 5})})
		  #{"cost" : "renyi", "beta" : 1.0, "alpha" : 0.9},
		  ],

"samples" : [3], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"batch_size_train" : 128,
"batch_size_eval" : 128,

"network_architecture" : [{
        "encoder" : [
            ("ConvNet2D", {"output_channels": [128, 256, 512, 1024], #
			   "strides" : [2],
			   "paddings" : ["SAME"],
			   "kernel_shapes" : [[5, 5]]},  1),
	    ("BatchFlatten", {}, 0),
            ("GaussianDiagonal", {"output_size" : 20,
                                  "module_tuple" : ("LinearWN", {"use_weight_norm" : False}),
				  "minimal_covariance": 0.0
                                  }, 0)],
 
        "decoder" : [
		  ("Linear", {"output_size" : 4 * 4 * 1024}, 1),
		  ("BatchReshape", {"shape" : (4, 4, 1024)}, 0),
                  ("ConvNet2DTranspose", {"output_channels": [512, 256, 128, 3],
	                                  "output_shapes" : [[8, 8], [16, 16], [32, 32], [64, 64]],
				          "strides" : [2],
				          "paddings" : ["SAME"],
				          "kernel_shapes" : [[5, 5]]},      0),
		  ("BatchFlatten", {}, 0),
                  ("LogitNormalDiagonal", {"module_tuple" : ("LinearWN", {"use_weight_norm" : False}),
				           "minimal_covariance": 0.0
                                          }, 0)
            ]
	}],
	
#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],


"weights_init" : [("contrib.layers.xavier_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],
#	       ("constant_initializer", {'value' : 0.01})],

"activation" : ["relu", "elu"],
"epochs" : 100,

},
{
"check_ops" : 0,

"save_model" : 1,
"n_epoch_save_model" : 10,

"save_summaries" : 1,
"n_epoch_save_summaries" : 1,

"ImagesReconstructHook" : {"period" : 10,
                            "n_images_columns" : 10,
                            "images_indexes" : {
                                                "train" : [0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900],
                                                "validation" : [0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900],
                                                },
                            },

"ImagesGenerateHook" : {"period" : 10,
                        "n_images_columns" : 6,
                        "n_gen_samples" : 36
                        },


#"LogpImportanceSamplingHook" : {"period" : 5,
#		       	        "n_samples" : [1,10,100],
#		     		"batch_size" : 128,
#                        	"repetitions" : 3
#                       	       },

"dirName" : "temp",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {2},
            "cores_per_GPU" : 1,
	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],


####### not used after this line

"n_epoch_log" : 1,
"n_epoch_plot" : 500,

# log norm and log
"log_gradient" : 0,#1,
"n_epoch_log_gradient" : 200,

# needs vae.sample()
"generate_images": 0,
"n_generate_images_rows": 2,
"n_generate_images_columns": 20,
"n_epoch_generate_images" : 250,

# needs x_train[subset, x_validate[subset, vae.regenerate()
"regenerate_images": 0,
"n_regenerate_images_rows": 2,
"n_regenerate_images_columns": 20,
"n_epoch_regenerate_images" : 250,

# needs x_train, x_validate, vae.encode()
"log_latent_vars_pca" : 0,
"n_epoch_log_latent_vars_pca_eigenvals" : 100,

# needs x_train[subset], x_validate[subset], 2 nodes "Distribution" of the latent
"log_latent_vars_model" : 0,
"n_epoch_log_latent_vars_model" : 25,
"log_latent_vars_model_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_latent_vars_model_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs x_train[subset], x_validate[subset], 2 nodes "Distribution" of the visible
"log_visible_vars_model" : 0,
"n_epoch_log_visible_vars_model" : 25,
"log_visible_vars_model_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_visible_vars_model_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs x_train[subset], x_validate[subset], vae.regenerate()
"log_repeated_reconstructions" : 0,
"n_epoch_log_repeated_reconstruction" : 25,
"n_repeated_reconstructions" : 100,
"log_repeated_reconstruction_points_train" : [10,20,30,40,50,100,200,300,400,500],
"log_repeated_reconstruction_points_test" : [10,20,30,40,50,100,200,300,400,500],

# needs pairs from x_train[subset], paris from x_validate[subset], 2 nodes "Distribution" of the latent, vae.decode() (attached to a feedable)
"log_latent_lin_interpolate" : 0,#1
"n_epoch_log_latent_lin_interpolate" : 100,
"lin_interpolate_couples" : [[10,20],[20,30]],
"n_interpolate_steps" : 20,

# needs x_train, y_train, x_validate, y_valudate, 2 nodes "Distribution" of the latent
"log_latent_vars_classification" : 0,
"n_epoch_log_latent_vars_classification" : 50,
"plot_misclassified_images" : 1,
"n_misclassified_images" : 100,

# needs x_train, x_validate, vae.encode()
"log_latent_vars_corr" : 0,
"n_epoch_log_latent_vars_corr" : 100,

# needs x_train, x_validate, vae.regenerate(), needs to pass a placefolder for the number of samples (using the placeho,der)
"log_estimate_log_p" : 0,
"n_epoch_log_estimate_log_p" : 100,
#"estimate_log_p_samples" : [1,10,50,200,1000],#,5000],
"estimate_log_p_samples" : [1,10,100,500,1000],#,1000,5000],  #,200,500,1000,2000,5000,10000
"estimate_log_p_batch_size" : 1000,
"estimate_log_p_repetitions" : 1,

# needs x_train, x_validate, vae.regenerate(), needs to pass a placefolder for the number of samples (using the placeho,der)
"log_estimate_function" : 0,

}
]
