# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "ThreeByThreeLines",
"binary" : 1,
"flip_fraction" : 0.,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"model" : ["HM.HM"],
"grad_clipping" : ("clip_by_norm", {"value" : 500}),
"clip_probs": 1e-3,
#"grad_clipping" : [x for x in [("clip_by_global_norm", {"value" : 100}),("clip_by_global_norm", {"value" : 10}), ("clip_by_value", {"value" : 10}), ("clip_by_norm", {"value" : 10}) ]],
"note" : "compare",
"stochastic" : [0],
"stochastic_noise_param" : [0.001], #0, ,0.00001,0.001,0.1,1

"denoising" : [0],

"optimizer": [(opt, {"learning_rate" : lr, "rescale_learning_rate": 1.0, "individual_learning_rate": ilr})
                        for opt in ["WakeSleepGradientDescentOptimizer"]
                        for lr in [1.0]
                        #for ilr in [1]#, [0.15,0.01,0.1]]
                        for ilr in [1]
             ],

"cost_function": [("HMLogJointLikelihood", {})
		  ],

"batch_size_train" : [s for s in [32]],
"batch_size_eval" : [s for s in [32]],

"samples" : [s for s in [10]], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [{
            "layers": ll,
        } for ll in [[6,1]]
	],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],

#"weights_init" : [("constant_initializer", {'value' : 0.1})],  # !!! truncated normal
"weights_init" : [("glorot_normal_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],

#"weights_init" : [("constant_initializer", {'value' : 0.01})],
#"bias_init" : [("constant_initializer", {'value' : 0.1})],

"activation" : ["elu"],
"epochs" : 400,

},
{
"check_ops" : 0,

# choose steps or epochs
"time_reference" : "epochs",
#"time_reference" : "steps",

# choose to save the model every n steps
"save_model" : 1,
"save_model_period" : 50, #0.1,

# how often to log stats (txt) and save summaries
"save_summaries" : 0,
"save_summaries_period": 5,
"stats_period" : 1,

# skips the first k stats for the plot
"plot_offset" : 1,


"GradientsHook" : {"period" : 1},

"ImagesInputHook" : {"period" : 20,
                     "how_many" : 20,
                     "n_images_columns" : 20},

"ImagesReconstructHook" : {"period" : 20,
                           "n_images_columns" : 16,
                           "images_indexes" : {
                                                 "train" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 "validation" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 },
                            },

"ImagesGenerateHook" : {"period" : 20,
                        "n_images_columns" : 10,
                        "n_gen_samples" : 100
                       },

"ThreeByThreeHook": {"period" : 1,
		             "sample_size": 10000},


"LogpImportanceSamplingHook" : {"period" : 20,
                                "n_samples" : [100],
                                "batch_size" : 1000,
                                "repetitions" : 1
                       	       },


"dirName" : "/ssd_data/csongor_test/testGDtest",
#"dirName" : "temp",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {0},
            "cores_per_GPU" : 8,

	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

}
]
