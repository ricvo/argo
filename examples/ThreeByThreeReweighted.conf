# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "ThreeByThreeLines",
"binary" : 1,
"flip_fraction" : 0.,
"vect" : False  # False if starting with convolutional, True will give the dataset already vectorialized (reshaped -1)
},
{

"model" : ["HM.HM"],
"grad_clipping" : (None, {}),
#"grad_clipping" : ("clip_by_norm", {"value" : 500}),
#"grad_clipping" : [x for x in [("clip_by_global_norm", {"value" : 100}),("clip_by_global_norm", {"value" : 10}), ("clip_by_value", {"value" : 10}), ("clip_by_norm", {"value" : 10}) ]],
"clip_probs": 1e-3,
				"note" : "ablation",
"stochastic" : [0],
"stochastic_noise_param" : [0.001], #0, ,0.00001,0.001,0.1,1

"denoising" : [0],

"optimizer": [(opt, {"learning_rate" : lr,  "individual_learning_rate": ilr, "diagonal_pad":dp, "rescale_learning_rate": rr, "q_baseline": True})
                        for opt in ["NaturalWakeSleepOptimizer","WakeSleepOptimizer","ReweightedWakeSleepOptimizer"]
						for lr in [(0.00001, "exponential_decay", {"learning_rate": 0.01,"decay_steps":1000,"decay_rate":0.99})]
						for rr in [1.0]
						for dp in [0.01]
                        #for ilr in [[0.15,0.01,0.1]]#, [0.15,0.01,0.1]]
                        for ilr in [1.0]#, [0.15,0.01,0.1]]
             ],

"cost_function": [("HMLogJointLikelihoodRWS", {})
		  ],

"batch_size_train" : [s for s in [32,100]],
"batch_size_eval" : [s for s in [100]],

"samples" : [s for s in [1,5,10]], # 1 or more
"covariance_parameterization" : "softplus", # "exp" or "softplus"

"network_architecture" : [{
            "layers": ll,
        } for ll in [[6,1]]
	],

#"weights_reg" : [("contrib.layers.l2_regularizer", {"scale" : 0.5})],
#"bias_reg" : [("contrib.layers.l2_regularizer",    {"scale" : 0.5})],

"weights_reg" : [None],
"bias_reg" :    [None],

#"weights_init" : [("constant_initializer", {'value' : 0.1})],  # !!! truncated normal
"weights_init" : [("glorot_normal_initializer",{})],  # !!! truncated normal
"bias_init" : [("constant_initializer", {'value' : 0.1})],

#"weights_init" : [("constant_initializer", {'value' : 0})],
#"bias_init" : [("constant_initializer", {'value' : 0})],

"activation" : ["elu"],
"epochs" :1000,

},
{
"check_ops" : 0,

# choose steps or epochs
"time_reference" : "epochs",
#"time_reference" : "steps",

# choose to save the model every n steps
"save_model" : 1,
"save_model_period" : 500, #0.1,

# how often to log stats (txt) and save summaries
"save_summaries" : 0,
"save_summaries_period": 5,
"stats_period" : 1,

# skips the first k stats for the plot
"plot_offset" : 1,



"GradientsHook" : {"period" : 1},

"ImagesInputHook" : {"period" : 50,
                     "how_many" : 20,
                     "n_images_columns" : 20,
					 "until": 1},

"ImagesReconstructHook" : {"period" : 5,
                           "n_images_columns" : 16,
                           "images_indexes" : {
                                                 "train" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 "validation" : [0,10,20,30,40,50,110,120,130,140,150,210,220,230,240,250],
                                                 },
                            },

"ImagesGenerateHook" : {"period" : 5,
                        "n_images_columns" : 10,
                        "n_gen_samples" : 100
                       },

"ThreeByThreeHook": {"period" : 1,
		             "sample_size": 10000},


"LogpImportanceSamplingHook" : {"period" : 50,
                                "n_samples" : [1,10,100],
                                "batch_size" : 320,
                                "repetitions" : 1
                       	       },


"dirName" : "/data1/csongor/HM/ablation",

"seed" : 0,
"runs" : 1,

"nodes" : [{"used_GPUs" : {4,5},
            "cores_per_GPU" : 4,

	    "IP": "localhost"   #  num_consumers = multiprocessing.cpu_count() * 2
	   }
	   ],

}
]
