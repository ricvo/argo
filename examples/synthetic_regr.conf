# add here few words about what is the purpose of this experiment
# keep only one experiment per file

##############################################################
[
{
"dataName" : "synthetic_regr",
"preproc": ['minmax','scale','sphere'],
"preproc": ['scale'],
"task" : "regression",
"stochastic" : [0],
"stochastic_noise_param" : [0],
# "learning_rate" : [0.001, 0.002],
"learning_rate" : [0.5,0.001,0.0001,0.00001,0.000001,0.0000001],
# "sample" : [1, 5, 10],
#"samples" : [1],
# "imp_sample" : [10, 20, 50],
#"imp_samples" : [200], #[200]
#"bit_flip" : [0, 0.001, 0.005, 0.01, 0.03],
"bit_flip" : [0],
# "bit_flip" : [0.001],
#"drop_out" : [0, 0.001, 0.005, 0.01, 0.03],
"drop_out" : [0],
#"drop_outs" : [0.001],
#"latent_noise" : [0, 0.0001, 0.0005, 0.001],
"latent_noise" : [0],
#"batch_size" : [100],
"batch_size" : [700],
"batch_size_test" : [300],
#"hidden_variables_sizes" : [200, 300, 400],
"hidden_variables_sizes" : [
			   [50,100,50,25] 
                           ],
#"latent_variables_size" : [10,13,15,17,20,23,26,30,35],
#"latent_variables_size" : [16,25,36],
#"regularizer" : [0,0.001,0.01,0.1,0.2,0.5],
"regularizer" : [0],
#"weights_init" : [VAE.xavier_init, VAE.normal_init, VAE.relu_init],
#"weights_init" : ["normal_init"],
#"weights_init" : ["xavier_init", "normal_init"],
"weights_init" : ["normal_init"],
#"transfer_fct" : ["relu"],
"activation_fct" : ["sigmoid","relu","elu"],
"epochs" : 60000,
},
{
"check_ops" : 0,
#"log_txt" : 0,

"save_model" : 1,
"n_epoch_save_model" : 500,

"log_gradient" : 0,
"n_epoch_log_gradient" : 10,

"log_predict_actual" : 0,
"n_epoch_log_predict_actual" : 1000,

"log_tf" : 0,
"verbose" : 1,
"dirName" : "./logs",

"seed" : 0,
"runs" : 1,
# not used anymore, to run on cpu, set used_GPUs = {-1} and then run single
# set pool size for GPU
"used_GPUs" : {0},
"cores_per_GPU" : 1   #  num_consumers = multiprocessing.cpu_count() * 2
}
]
